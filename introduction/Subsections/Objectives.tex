\section{Objectives}

Can we learn deep audio embeddings from reordering and scrambling sequences of musical information and adding audio effects to improve unsupervised music boundary detection?

This study explores music's high-level tonal structure by leveraging self-supervised deep neural networks, inductive bias, and aural skills to learn and analyze music embeddings applicable to subsequent MIR tasks, in our particular case, music boundary detection.

The study aims to develop and learn a unified numerical understanding (or embedding \footnote{A learned representation or embedding is a numerical output -usually a fixed-size vector- produced by a machine-learning model. Good representations should be versatile across various tasks and require limited supervision \cite{Turian2022HEAR:Representations}.}) for a music piece that integrates high-level elements like melody contour, harmony relationships, and other comprehensive components such as the relationships between elements unfolding through time. 

The research will utilize MIR computational methodologies and musicological perspectives to explore musical compositions from various styles and genres. 

This approach seeks to replicate human auditory capabilities in understanding and identifying abstract and semantic musical elements independent of their sonic qualities.