\section{Objectives}

Can deep audio embeddings be learned from reordering and scrambling sequences of musical information, and can audio effects be added to improve unsupervised music boundary detection?

This study explores the high-level tonal structure of music by leveraging self-supervised deep neural networks, inductive bias, and aural skills to learn and analyze music embeddings that can be applied to subsequent MIR tasks, with a particular focus on music boundary detection.

This study aims to develop and learn a unified numerical understanding (or embedding \footnote{A learned representation or embedding is a numerical output - usually a fixed-size vector - produced by a machine-learning model. Good representations should be versatile across various tasks and require limited supervision \cite{Turian2022HEAR:Representations}.}) for a piece of music that integrates the high-level relationships between musical elements as they unfold over time.

The research will employ MIR computational methodologies and musicological perspectives to explore musical compositions across various styles and genres.

This approach seeks to replicate human auditory capabilities by understanding and identifying abstract and semantic musical elements independent of their sonic qualities.