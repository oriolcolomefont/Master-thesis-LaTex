\section{Objectives}

This study explores music's high-level tonal structure by leveraging self-supervised deep neural networks, inductive bias, and aural skills to learn and analyze music embeddings applicable to subsequent MIR tasks. 

The study aims to develop a unified embedding, $e(M)$, for a music piece $M$ that integrates high-level elements like melody contour, harmony relationships, and other comprehensive components, arranging the embedding space to emphasize clustering of these sophisticated musical content without any consideration for sonic quality attributes. Utilizing a blend of Music Information Retrieval (MIR) computational methodologies and musicological perspectives, the research will explore musical compositions from various styles and genres. 

This approach seeks to replicate human auditory capabilities in understanding and identifying abstract and semantic musical elements independent of their sonic qualities.