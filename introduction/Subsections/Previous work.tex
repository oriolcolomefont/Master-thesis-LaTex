\section{Music boundary detection as a downstream task}

While the usefulness of similar self-supervised learned embeddings can be evaluated in countless downstream tasks \cite{Li2023MERT:Training, Kim2020OneStrategies}, we have chosen music boundary detection. Also known as track segmentation and commonly tackled with spectral analysis, it is a subset of music structure analysis (MSA) suitable for its popularity \cite{Smith2013ATask}, complexity, and product-compelling nature. 

This interdisciplinary field aims to understand the structure of music \cite{Nieto2020Audio-BasedApplications}. Due to subjectivity, ambiguity, and data scarcity, audio-based MSA faces non-solved challenges like boundary placement ambiguity and similarity quantification \cite{NietoPerceptualMusic}.  

\subsection{Related work}

Related research investigates audio embeddings derived through unsupervised methods to enhance the performance of music segmentation algorithms \cite{deepfeaturesegment, SalamonDeepSegmentation}. Both studies leverage the power of deep learning and data-driven feature learning, presenting advancements over traditional manually-engineered methods.

The study in \cite{deepfeaturesegment} presents a novel approach for music segmentation, utilizing audio embeddings learned via few-shot learning and a music auto-tagging model. This method, which replaces the traditionally handcrafted MFCC and CQT features, significantly improves multi-level music segmentation, achieving state-of-the-art results and outperforming existing baselines.

While \cite{deepfeaturesegment} approach and ours are very similar, there are some contrastive differences worth mentioning. These nuances stem from how much the approaches are task-tailored: the authors train their model on a dataset of audio features labeled with their corresponding music segments. They use a sampling strategy to create positive and negative examples likely from the same or different music segments. The method by \cite{deepfeaturesegment} requires music with clear beats and onsets. The sampling strategy relies on the fact that beats or onsets typically separate music segments. If the music does not have clear beats or onsets, it will be difficult for the method to create positive and negative examples likely to be from the same or different music segments.

Our approach is slightly different in two ways. First, we use physical time as input data rather than audio features. Second, our triplet selection pipeline is more abstract and designed for broader music understanding scope ---yet still to be evaluated--- rather than specifically tailored to music segmentation.

On the other hand, \cite{SalamonDeepSegmentation} explores using unsupervised deep learning methods for creating meaningful music representations and applying them to music structure analysis tasks. The research employs the same kind of deep architecture trained on millions of tracks from a music streaming service. The study reveals that these embeddings, derived via unsupervised learning, are effective at capturing musical structures, particularly when integrated with traditional feature engineering methods and a multi-level section fusion algorithm to merge short sections into longer ones. 

Both studies emphasize the significant potential of deep learning-based feature learning in enhancing music segmentation tasks, which have traditionally relied on manual feature engineering methods only. Furthermore, they demonstrate how using deep learning techniques improves the performance of segmentation algorithms and enables a more nuanced and detailed analysis of music structure. 

The results prove that unsupervised deep-learning techniques which derive audio embeddings offer more robust and efficient alternatives to exclusively traditional manual feature-engineering methods in music segmentation.

