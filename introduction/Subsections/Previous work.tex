\section{Related work}



Prior research has underlined the importance of choosing audio features to identify and label distinct music segments in music segmentation. However, acquiring music segment annotation data for learning feature transformations has been a limiting factor due to its time-consuming and costly nature. To overcome this, studies have employed unsupervised deep learning, using abundantly available music audio. Combining spectrograms with self-similarity lag matrices has captured a broader spectrum of structural information. This approach has significantly improved the performance of segmentation algorithms, achieving state-of-the-art results. Additionally, strategies considering the hierarchical nature of the structural organization, utilizing two-level annotations of main and secondary boundaries, have further enhanced boundary recognition performance \cite{unsupervisedlearndeepfeat, GrillMUSICANNOTATIONS, Hernandez-Olivan2021MusicFeatures, SalamonDeepSegmentation}.

Aside from the music structure applications, recent advancements in neural networks have enabled the development of low-dimensional embeddings which encapsulate critical musical attributes, thus facilitating the computation of diverse task-specific elements. Such tasks encompass developing an audio embedding method that excels across a wide array of applications without fine-tuning \cite{Turian2022HEAR:Representations}, improving environmental \cite{Kim2020OneStrategies} sound classification \cite{CramerLOOKEMBEDDINGS}, enhancing vocal-centric music tasks via cross-domain audio embedding \cite{Kim2021LearningLoss}, combining task-specific and pre-trained features for improved audio classification \cite{Hung2022Feature-informedClassification}, creating a music similarity search engine tailored for video producers \cite{epidemic}, enhancing Music Emotion Recognition (MER) performance \cite{KohComparisonRecognition}, investigating the efficacy of speaker recognition pre-trained model embeddings \cite{lightweight}, embedding songs for similarity comparison via artist identification \cite{contentmusicsimtriplet2020}, solving the cross-modal text-to-music retrieval issue \cite{WonEmotionStories}, and facilitating automated music rearrangement \cite{Stoller2018IntuitiveTransitions, Plachouras2023MusicSegmentation}.

Additionally, deep audio embeddings offer the benefit of transferability, acting as a starting point for various tasks, thus saving computational resources and time as opposed to training a model from scratch \cite{HamelTransferSimilarity}. 

Considering their proven promising efficacy in the existing literature and their potential for transfer learning, it appears appealing to investigate further deep audio embeddings for MIR downstream tasks. Especially as it is still an open question whether a general-purpose audio representation can effectively mimic human hearing \cite{Turian2022HEAR:Representations} even though some approaches achieved to generalize up to fourteen music understanding tasks \cite{Li2023MERT:Training}.
