\section{Related work}

Related research \cite{deepfeaturesegment, SalamonDeepSegmentation} investigates the use of audio embeddings derived through unsupervised methods to enhance the performance of music segmentation algorithms. Both studies leverage the power of deep learning and data-driven feature learning, presenting advancements over traditional manually-engineered methods.

The study in \cite{deepfeaturesegment} presents a novel approach for music segmentation, utilizing audio embeddings learned via Few-Shot Learning and a music auto-tagging model. This method, which replaces the traditionally handcrafted MFCC and CQT features, significantly improves multi-level music segmentation, achieving state-of-the-art results and outperforming existing baselines.

On the other hand, \cite{SalamonDeepSegmentation} explores using unsupervised deep learning methods for creating meaningful music representations and applying them to tasks like music structure analysis. The research employs Convolutional Neural Networks (CNNs) trained on millions of tracks from a music streaming service. The study reveals that these embeddings, derived via unsupervised learning, are effective at capturing musical structures, particularly when integrated with traditional feature engineering methods.

Both studies emphasize the significant potential of deep learning-based feature learning in enhancing music segmentation tasks, which have traditionally relied on manual feature engineering methods. Furthermore, they demonstrate how using deep learning techniques improves the performance of segmentation algorithms and enables a more nuanced and detailed analysis of music structure. 

The results prove that unsupervised deep learning techniques that derive audio embeddings offer a robust and more efficient alternative to traditional manual feature engineering methods in music segmentation.

