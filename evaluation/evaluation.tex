\chapter{Evaluation}

\section{Datasets and metrics}
\label{section:Datasets and metrics}

The developed model has been trained on GTZAN \cite{GTZAN}, the Million Song Dataset (MSD) \cite{MSD} and evaluated on the SALAMI dataset \cite{Smith2011DESIGNANNOTATIONS}. 

\subsection{The GTZAN dataset}
The GTZAN Genre Collection is a widely-used dataset for music genre classification tasks. It comprises 1,000 audio tracks that are each 30 seconds long. These tracks are evenly distributed across ten genres: blues, classical, country, disco, hip hop, jazz, metal, pop, reggae, and rock, each containing precisely 100 songs. The audio files are stored as WAV files with a sample rate of 22,050 Hz \cite{GTZAN}.

\subsection{The Million Song Dataset}
The Million Song Dataset (MSD) is a publicly available audio and metadata collection for a million contemporary popular music tracks.

The MSD encourages research on large-scale recommendation systems, exploration of musicological properties, and general research on large datasets. It provides a massive scale and diversity of data, making it an excellent resource for complex, large-scale music-related machine-learning tasks \cite{MSD}.

\subsection{The SALAMI dataset}

The Structural Analysis of Large Amounts of Music Information (SALAMI) project conducts extensive structural analyses on a wide variety of music. SALAMI segments music pieces into distinct sections, integrating different analyses, including perceptual, functional, and transcriptional. While there are some limitations, this approach offers a nuanced and thorough understanding of musical structure.

SALAMI covers an extensive range of music genres and styles, including but not limited to classical, jazz, popular, and world music. These pieces originate from diverse sources, including Codaich, the Internet Archive's Live Music Archive, the RWC Music Database, and the Isophonics database.

Each piece of music in SALAMI is accompanied by detailed metadata such as title, artist, duration, names of the annotators, and the time taken for the annotation process. This metadata is provided in multiple formats, catering to each source database's needs.

While SALAMI does not directly distribute audio, it directs users to corresponding audio files on streaming platforms. This makes it a valuable resource for researchers working on music structure analysis, genre classification, music summarization, and other related fields \cite{Smith2011DESIGNANNOTATIONS}.

Note: We used SALAMI's original release from 2011, featuring 1,359 tracks.

\subsection{Metrics}

Three primary metrics have been utilized to assess the model performance: Precision, Recall, and F-measure. 

\subsubsection{Precision}

Precision quantifies the proportion of accurately identified boundaries relative to all estimated boundaries to indicate the algorithm's accuracy in boundary detection.

\begin{equation}
\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
\end{equation}

\subsubsection{Recall}

On the other hand, recall measures the proportion of accurately detected boundaries against all reference boundaries, indicating the completeness or sensitivity of the algorithm.

\begin{equation}
\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
\end{equation}

\subsubsection{F-measure}

Lastly, the F-measure provides a harmonized measure of precision and recall. As a widely adopted metric for boundary detection, it compares predicted boundaries with ground truth ones, yielding a score between 0 and 1. This score, calculated as the harmonic mean of Precision and Recall, effectively accounts for both under-segmentation and over-segmentation. Given the inherent inaccuracies in human annotations and prediction errors, the F-measure allows for minor deviations between predicted and actual boundaries. The F-measure can adjust the tolerance threshold, permitting a predicted boundary to be considered correct if it falls within a predefined window of a ground truth boundary \cite{NietoPerceptualMusic, Turnbull2007ABOOSTING}. In this work, we opted for an F-measure tolerance of 0.5 seconds, a decision largely guided by established norms in existing literature and music information retrieval research. This 0.5-second tolerance balances precision and flexibility, demanding accurate boundary predictions while accommodating slight variations inevitable due to the subjective nature of music segmentation.

We computed the F-measure for each track and then calculated the average rather than aggregating all tracks into a single dataset and calculating the F-measure on this combined set.

\begin{equation}
\text{F-measure} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\newpage