
METRICS

\begin{itemize}
    \item \textbf{Precision}: Precision measures the proportion of correctly detected boundaries out of the total estimated boundaries. It quantifies the accuracy of the boundary detection algorithm. It is calculated as:
    
    \[
    \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
    \]
    
    High precision indicates a low rate of false positives, meaning the algorithm correctly identifies boundaries without many incorrect detections.
    
    \item \textbf{Recall}: Recall measures the proportion of correctly detected boundaries out of the total reference boundaries. It quantifies the completeness or sensitivity of the boundary detection algorithm. It is calculated as:
    
    \[
    \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
    \]
    
    High recall indicates a low rate of false negatives, meaning that the algorithm successfully detects most of the true boundaries without missing many.
    
    \item \textbf{F-measure}: The F-measure is a widely used boundary detection metric computed by comparing the predicted and ground truth boundaries. This metric, ranging from 0 to 1, represents the harmonic mean of precision and recall, providing a balanced view of under-segmentation and over-segmentation. Given the potential inaccuracies in human annotations and prediction errors, the F-measure tolerates minor discrepancies between predicted and ground truth boundaries. This tolerance level can be adjusted, enabling the metric to consider a predicted boundary as correct if it's within the set tolerance window of a ground truth boundary \cite{Turnbull2007ABOOSTING}. The F-measure is calculated as follows:
    
    \[
    \text{F-measure} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \]
\end{itemize}

Extend table by Hernandez-Olivan, C., Beltran, J. R. \& Diaz-Guerra, D. 
\cite{Hernandez-Olivan2021MusicFeatures}

\begin{table}[h]
\centering
\small
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{4.5cm}XXXXX}
\toprule
\thead{\centering\textbf{Year, Authors, Ref}} & \thead{\centering\textbf{Algorithm}} & \thead{\centering\textbf{Input}} & \thead{\centering\textbf{Method}} & \thead{\centering\textbf{F-measure}} \\
\midrule
\addlinespace
2012, Kaiser et al., [27]& KSP2 & SSM & Novelty measure  & 0.286 \\
\addlinespace
2013, McFee \& Ellis, [20] & MP2 & MLS & Fisher’s Linear Discriminant  & 0.317 \\
\addlinespace
2014, Nieto \& Bello, [28] & NB1 & MFCCs, chromas & Checkerboard-like kernel  & 0.299 \\
\addlinespace
2015, Cannam et al., [29] & CC1 & Timbre-type histograms & HMM  & 0.213 \\
\addlinespace
2016, Nieto, [30] & ON2 & Constant-Q Transform Spectrogram & Linear Discriminant Analysis  & 0.299 \\
\addlinespace
2017, Cannam et al., [29] & CC1 & Timbre-type histograms & HMM  & 0.212 \\
\addlinespace
2007, Turnbull et al., [19] & - & MFCCs, chromas, spectrogram & Boosted Decision Stump  & 0.378 \\
\addlinespace
2011, Sargent et al., [34] & - & MFCCs, chromas & Viterbi  & 0.356 \\
\addlinespace
2014, Ullrich et. al, [22] & - & MLS & CNN  & 0.465 \\
\addlinespace
2015, Grill \& Schlüter, [4] & - & MLS, SSLMs & CNN  & 0.523 \\
\addlinespace
2015, Grill \& Schlüter, [5] & - & MLS, PCPs, SSLMs & CNN  & 0.508 \\
\addlinespace
2017, Hadria \& Peeters, [35] & - & MLS, SSLMs & CNN  & 0.291 \\
\bottomrule
\end{tabularx}
\caption[Baseline. State of the art table.]{Your Caption.}
\label{tab:my_label}
\end{table}