\section{Implementation details}

Our approach could be broadly categorized as cognitive modeling, as it strives to mimic human cognitive processes and problem-solving through a computerized model. We champion exposure-based learning in music and active engagement with musical styles, genres, techniques, and learning methods to foster well-rounded musical proficiency and efficient performance with novel data in practical applications.

\subsection{Contrastive Learning of Musical Representations}

Piaget's theory of cognitive development suggests children acquire knowledge through sensory experiences, gradually developing abstract reasoning and schemas—basic cognitive structures \cite{Huitt2003PiagetsDevelopment}. These schemas evolve by integrating new information through assimilation and accommodation \cite{audioselfsupsurvey}.

In pattern recognition, models are designed to be robust against known invariances—input data transformations—allowing consistent output. Conversely, unknown invariances not considered in the model's design can still be handled due to its learning capacity.

Contrastive Learning of Musical Representations (CLMR) learns valuable, discriminative music representations without explicit labels by contrasting positive augmentations of a musical piece against negative ones \cite{CLMR2021}. CLMR works under a machine learning subset called Self-Supervised Learning (SSL), where models self-learn from unlabeled data, creating their supervisory signal \cite{audioselfsupsurvey}. This mimics human learning from observations and interactions, transforming unsupervised problems into supervised ones by auto-generating labels. The benefits of SSL include reduced reliance on labeled data, making data representation more robust and generalizable.

The CLMR methodology involves creating different augmentations of the same musical piece as positive pairs and using others as antagonistic pairs. Then, training by minimizing a contrastive loss function encourages the model to produce similar representations for positive pairs and dissimilar representations for antagonistic pairs.


\subsection{Triplet Siamese Networks}

For this task, a Triple Siamese Network is suggested, a model architecture proved efficient for music similarity retrieval tasks \cite{contentmusicsimtriplet2020}, intending to minimize the loss function between an anchor, positive, and negative sample, achieved through online triplet mining \cite{Sikaroudi2020OfflinePatches}. The overarching aim is to train a model in a self-supervised setting, which can effectively distinguish between similar and dissimilar samples. 

A Siamese Network is a deep learning architecture introduced by Bromley and LeCun in the early 1990s \cite{Bromley1993SignatureNetwork}. This architecture is explicitly designed for tasks requiring comparing or assessing similarity between two input instances. It comprises two or more identical subnetworks connected in parallel and joined at the output layer. These subnetworks share the same architecture, weights, and hyperparameters, allowing more efficient memory usage and computational complexity.

Siamese Networks are especially effective for learning from limited or imbalanced datasets, as they focus on learning a similarity metric rather than the specific features of individual classes. Each subnetwork processes an input independently, and its outputs are then combined and further processed to yield a single similarity score. The shared weights during training enable the model to learn an invariant representation for the input instances, thereby enhancing its efficiency in comparing and contrasting them. To achieve this, Siamese Networks employ specialized loss functions, such as contrastive or triplet loss, which aim to minimize the distance between similar input pairs and maximize the distance between dissimilar ones.

An extension of this architecture is the Triplet Siamese Network, which involves comparing three input instances instead of two. This network aims to learn an embedding space in which similar instances are close to each other and dissimilar instances are distant. The input to this network consists of an anchor, a positive instance (from the same class as the anchor), and a negative instance (from a different class). Each instance is processed by an identical subnetwork that shares its architecture, weights, and hyperparameters with the others.

\subsubsection{Encoder: SampleCNN}

The SampleCNN model \cite{Lee2018SampleCNN:Classification} is a CNN designed for raw waveform audio data, treating each audio sample as an independent channel and applying 1-dimensional convolution along the temporal axis. Its implementation is an adaptation of \cite{CLMR2021} using PyTorch \cite{Paszke2019PyTorch:Library} and PyTorch Lightning \cite{PyTorchDocumentation}.

This fully convolutional model reduces computational requirements and learns features at different scales through its multi-resolution architecture \ref{tab:samplecnn}.

\begin{table}[h]
\centering
\small
\begin{tabularx}{\textwidth}{>{\hsize=1.6\hsize}X>{\hsize=0.6\hsize}X>{\hsize=0.6\hsize}X}
\toprule
\thead{\textbf{Layer Type}} & \thead{\textbf{In Channels}} & \thead{\textbf{Out Channels}} \\
\midrule
Conv + ReLU & 1 & 128 \\
\addlinespace
\multicolumn{3}{c}{The following layers are repeated depending on the strides and hidden parameters:} \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 128 & 128 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 128 & 128 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 128 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 512 \\
\addlinespace
Conv + BatchNorm + ReLU + AvgPool & 512 & 512 \\
\addlinespace
Fully connected (representation layer) & 512 & 128 \\
\bottomrule
\end{tabularx}
\caption{Layer specifications for SampleCNN model}
\label{tab:samplecnn}
\end{table}


The original model has been modified slightly to serve our specific needs: the output from the final convolutional layer is now subjected to an average pooling operation at the end.

%Colour CNN
\input{figures/neural networks/colour CNN}

\subsection{Optimizer and learning rate}

In our research, we've employed the AdamW optimizer \cite{Loshchilov2017DecoupledRegularization}, an optimized variant of the widely-used Adam optimizer for training neural networks. AdamW adeptly balances the learning rate across network weights, providing an efficient strategy for weight decay management by isolating it from gradient updates. 

\subsubsection{Learning rate}

The learning rate, set at 0.003, is a pivotal parameter dictating the step size at each iteration towards loss function minimization. It's a delicate balancing act— a high rate promises swift convergence with a risk of minimum overshoot, while a lower rate provides careful convergence but necessitates more iterations.


\subsection{Audio augmentation and transformation pipeline}

Conventional transformations, such as log-scaled Mel spectrograms or Mel Frequency Cepstral Coefficients, capture key features but may obscure some information inherent in raw audio data.

Raw audio data offers unique advantages. Primarily, it ensures the retention of the original signal, which could reveal novel findings, and facilitates the direct extraction of features through advanced deep learning models \cite{learning}. However, it is not without challenges: high sample rates result in high dimensionality, demanding substantial computational resources. Additionally, raw audio is noise-sensitive, complicating the extraction process. Features derived from raw audio also often lack the interpretability of traditional ones \cite{Schindler2020DeepTutorial}. Hence, the choice of input data depends on the specific task, available computational resources, and the balance between data preservation and computational efficiency.

Time-domain processing, a feature of our experiments, naturally handles temporal patterns and sequences in data, thereby avoiding windowing artifacts. Although Mel-spectrogram-based methods are effective for various audio-related machine learning tasks, their limitations lie in representing perceptual similarity. Spectrograms capture frequency distribution over time. Yet, the complexity of human auditory perception—encompassing temporal patterns, phase relationships between frequencies, and higher-level musical structures—means that musically similar sounds can have distinct spectrograms. This discrepancy implies that using spectrogram distance alone for measuring high-level music similarity may not always align with human perceptions \cite{Kim2020OneStrategies}\cite{Mesostructures2023}.

\subsubsection{Positive sample generation chain}

The positive image in our study must preserve its intelligible content when subjected to temporal (or any) transformations, regardless of alterations in sonic qualities. Maintaining the temporal structure and meaningfulness of the content allows it to present musical elements identical or strikingly similar to the original track.

While we have experimented with helpful audio augmentation tools such as \cite{Spijkervet2021Spijkervet/torchaudio-augmentations:V1.0} and \cite{Kharitonov2020DataDomain}, the specific requirements of our experiments necessitated the development of our transformation chain using \textit{torchaudio's} \cite{Yang2021TorchAudio:Processing} implementation of \textit{SoX} \cite{sox}: given an anchor audio signal $A[n]$, we generate a positive signal $P[n]$ by applying a series of amplitude, time-domain, frequency-domain, modulation, reverberation, and nonlinear effects with additive noise on top of it. 

In this analysis, various parameters represent different effects on the signal. We use $g \in [-12, 0]$ for gain, $\Delta p \in [-1200, 1200]$ for pitch shift, $h_R[n]$ for the reverb impulse response, $h_C[n]$ for the chorus impulse response, $d \in [0, 30]$ for overdrive, $\alpha \in [0.9, 1.1]$ for speed change, $\beta \in [0.9, 1.1]$ for stretch factor, $f_m \in [0.1, 100]$ for tremolo's modulation frequency, and $d_m \in [1, 101]$ for tremolo's depth. Each effect is applied aggressively but does not disrupt the high-level musical content.

\textbf{Amplitude effects}, like gain, modify the signal's overall amplitude with a constant factor $g$. 

\textbf{Time-domain effects} such as speed change and stretch alter the signal's playback speed and duration without changing its pitch, respectively. The speed and stretch factors are $\alpha$ and $\beta$.

\textbf{Frequency-domain effects} adjust the frequency content or pitch of the signal, as in pitch shifting, which alters the pitch by $\Delta p$. 

\textbf{Nonlinear effects} such as overdrive add harmonic distortion using the parameter $d$.

\textbf{Modulation effects}, including chorus and tremolo, use a control signal or low-frequency oscillator. The specified ranges determine the chorus parameters, and the tremolo's amplitude modulation frequency and depth are $f_m$ and $d_m$.

\textbf{Reverberation effects} simulate physical space's acoustic reflections and reverberations, like reverb that applies an impulse response $h_R[n]$. Finally, \textbf{Noise effects} add a noise signal $N[n]$ with a signal-to-noise ratio (SNR) in the range $[12, 100]$.

The positive signal $P[n]$ is generated according to the equation given below, where the symbols denote specific effects as indicated:

\begin{equation}\label{eq:positive_signal}
P[n] = A[n] \ast h_{G}[n] \ast h_{C}[n] \ast h_{D}[n] \ast h_{P_t}[n] \ast h_{R}[n] \ast h_{S}[n] \ast h_{T}[n] \ast h_{T_m}[n] + N[n]
\end{equation}

The $\ast$ symbol represents convolution, and the various $h$ symbols indicate the impulse responses of specific effects. The noise signal $N[n]$ is included with a specific SNR.

\subsubsection{Negative sample generation}

Circling back to our premises and assumptions, we posit that high-level musical content unfolds over time; therefore, we argue that the temporal structure of the negative images, in contrast to our anchor, should be disrupted. While maintaining similar sonic qualities, the content should be rendered unintelligible.

\begin{enumerate}
\item Calculate the minimum and maximum audio chunk lengths in samples:
\begin{equation}
l_{min} = t_{min} \times S
\end{equation}
\begin{equation}
l_{max} = t_{max} \times S
\end{equation}

The minimum duration $t_{min}$ is set to 0.05 seconds, and the maximum duration $t_{max}$ is set to 1 second. This range is chosen thoughtfully to strike a balance between the two factors. On the one hand, it is above the just noticeable difference (JND) threshold, the smallest change in a stimulus that can be perceived. On the other hand, it is short enough to maintain a reasonably-sized window to avoid discernible musical content \cite{Fastl2007Just-NoticeableChanges}.

\item Generate random audio chunk lengths $l_1, l_2, \ldots, l_{n-1}$ from the uniform distribution on the interval $[l_{min}, l_{max}]$. Calculate the final audio chunk length as:
\begin{equation}
l_n = L_A - \sum_{i=1}^{n-1} l_i
\end{equation}
where $L_A$ is the length of the anchor signal in samples.
\item Split the anchor signal $A$ into audio chunks $C_1, C_2, \ldots, C_n$ according to the calculated audio chunk lengths.
\item Shuffle the audio chunks randomly to get the permuted slices $C_{\sigma(1)}, C_{\sigma(2)}, \ldots, C_{\sigma(n)}$, where $\sigma$ is a random permutation of indices from $1$ to $n$.
\item Concatenate the shuffled audio chunks to generate the negative signal:
\begin{equation}\label{eq:negative_signal}
N = C_{\sigma(1)} \oplus C_{\sigma(2)} \oplus \ldots \oplus C_{\sigma(n)}
\end{equation}
\end{enumerate}

\subsection{Loss function}

Schroff, F., Kalenichenko, D., and Philbin, J. at Google initially proposed and used triplet loss to learn face recognition of the same person at different poses and angles. \cite{Schroff2015FaceNet:Clustering}

The triplet loss function used in Siamese-like networks guides the learning process. It seeks to minimize the distance between the anchor and positive instances while maximizing the distance between the anchor and the negative instances. A margin parameter is included in the loss function to ensure a minimum separation between positive and negative instances in the embedding space.

The triplet loss function $\mathcal{L}(\mathbf{a}, \mathbf{p}, \mathbf{n})$ aims to ensure that an anchor vector $\mathbf{a}_i$ is closer in the embedding space to a positive vector $\mathbf{p}_i$ (representing an example of the same class) than to a negative vector $\mathbf{n}_i$ (representing an example of a different class) by at least a margin $m$. It is calculated by summing the losses overall $N$ triplets in the dataset, where the equation gives the loss for each triplet:

\begin{equation}
\mathcal{L}(\mathbf{a}, \mathbf{p}, \mathbf{n}) = \sum_{i=1}^{N} \max \left(0, \left| \mathbf{a}_i - \mathbf{p}_i \right|_2^2 - \left| \mathbf{a}_i - \mathbf{n}_i \right|_2^2 + m \right)
\end{equation}

The final loss used for model training is then the average loss over a mini-batch of $N$ triplets:

\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(\mathbf{a}_i, \mathbf{p}_i, \mathbf{n}_i)
\end{equation}

As previously stated, the goal of minimizing this loss function is to learn discriminative embeddings, where similar examples are grouped closely together. In contrast, dissimilar examples are placed farther apart in the embedding space.

While some packages can be found in the MIR literature \cite{auraloss}, we wrote our own PyTorch \cite{Paszke2019PyTorch:Library} implementation for the sake of our experiments.

\subsection{Online triplet mining and batch normalization}

Online triplet mining is beneficial for managing large datasets by dynamically selecting the most informative triplets during training, focusing on each mini-batch. This strategy makes the process memory-efficient by negating the need to store all possible triplet combinations. Still, it also enhances model performance through quicker convergence by focusing on challenging examples based on the current model state.

This hard triplet mining selects triplets $(a, p, n)$ to maximize the Euclidean distance between the anchor and positive samples and the anchor and negative samples. These distances, $D_{\text{AP}}$ and $D_{\text{AN}}$, are computed respectively as:


\begin{align}
D_{\text{AP}} &= \sqrt{\sum_{i} (A_i - P_i)^2} & D_{\text{AN}} &= \sqrt{\sum_{i} (A_i - N_i)^2}
\end{align}


\subsubsection{Batch normalization}

In implementing the batch normalization step, it is necessary to standardize the audio lengths across all elements in the minibatch. We opted to zero-pad all clips to the length of the longest clip, valuing data integrity and completeness over potential performance trade-offs. Thus, the length of the longest array in the batch, which sets the standard for all others, is as follows:

\begin{equation}
L_{\text{max}} = \max_{i \in I} \left( \max \left( |A_i|, |P_i|, |N_i| \right) \right)
\end{equation}

\begin{itemize}
    \item $I$ represents the set of all items in the batch.
    \item $|A_i|$, $|P_i|$, and $|N_i|$ denote the lengths of the anchor, positive, and negative embedding vectors for the $i$-th item, respectively.
    \item The $\max$ function is applied to find the longest of these three lengths for each item, and then the maximum of these maximum lengths is taken over all items in the batch. This gives the maximum length, $L_{\text{max}}$, of any vector in the batch.
    \end{itemize}

\newpage