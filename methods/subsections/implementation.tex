\section{Implementation details}

Our approach falls broadly under cognitive modeling, which seeks to simulate human cognitive processes and problem-solving through a computerized model. We advocate for exposure-based learning in music, encouraging active engagement with various musical styles, genres, techniques, and learning methods. This approach promotes comprehensive musical proficiency and efficient performance when encountering novel data in practical applications.

According to Piaget's theory of cognitive development, children gain knowledge through sensory experiences and gradually develop abstract reasoning and schemas—basic cognitive structures \cite{Huitt2003PiagetsDevelopment}. These schemas evolve by incorporating new information through processes of assimilation and accommodation \cite{audioselfsupsurvey}.

In pattern recognition, models are designed to exhibit robustness against known invariances—transformations of input data—thereby ensuring consistent output. Interestingly, even unknown invariances not explicitly considered in the model's design can be accommodated due to the model's inherent learning capacity.

Contrastive Learning of Musical Representations (CLMR) is a method that learns valuable, discriminative music representations without explicit labels by contrasting positive augmentations of a musical piece against negative ones \cite{CLMR2021}. CLMR falls under a subset of ML called self-supervised learning (SSL) \cite{Balestriero2023ALearning}, where models learn independently from unlabeled data to create their supervisory signals \cite{audioselfsupsurvey}. This method resembles how humans learn from observations and interactions, turning unsupervised problems into supervised ones by auto-generating labels. Benefits of SSL include reduced dependence on labeled data, which contributes to more robust and generalizable data representations.

The methodology of CLMR involves generating different augmentations of the same musical piece to serve as positive pairs while using other pieces as antagonistic pairs. The training process, which minimizes a contrastive loss function, encourages the model to produce similar representations for positive and dissimilar ones for antagonistic pairs.

Although SSL has proven effective in speech and audio, its application to music audio remains relatively unexplored. This is primarily due to the unique challenges of modeling musical knowledge, especially regarding music's tonal and pitched characteristics \cite{Li2023MERT:Training}.

\subsection{Triplet Siamese Networks}

This work suggests employing a Triple Siamese Network, a model architecture known for its efficacy in music similarity retrieval tasks \cite{contentmusicsimtriplet2020}. The aim is to minimize the loss function between a triplet of anchor, positive, and negative samples, utilizing online triplet mining for optimizing memory resources \cite{Sikaroudi2020OfflinePatches}. This self-supervised learning (SSL) model aims to distinguish between similar and dissimilar samples effectively.

Introduced by Bromley and LeCun \cite{Bromley1993SignatureNetwork}, Siamese Networks are deep learning architectures designed for tasks requiring comparison or similarity assessment between instances. The architecture comprises identical subnetworks that share the same parameters, improving memory usage and computational efficiency. Rather than learning specific features of individual classes, they focus on a similarity metric, making them ideal for limited or imbalanced datasets. Each subnetwork processes an input independently, combining the outputs to yield a similarity score. Training with shared weights enables the model to learn invariant input representation, improving comparison efficiency. This is accomplished through specialized loss functions such as contrastive or triplet loss, aiming to minimize the distance between similar inputs and maximize it for dissimilar ones.

The Triple Siamese Network extends this architecture, comparing three input instances instead of two. It aims to learn an embedding space wherein similar instances are closer and dissimilar ones are distant. The inputs, consisting of an anchor, a positive instance (similar to the anchor), and a negative instance (dissimilar to the anchor), are processed by identical subnetworks sharing the same parameters.

\subsubsection{Encoder architecture: SampleCNN}

The SampleCNN model \cite{Lee2018SampleCNN:Classification} is a CNN designed for raw waveform audio data, treating each audio sample as an independent channel and applying 1-dimensional convolution along the temporal axis. Its implementation is an adaptation of \cite{CLMR2021} using \textit{PyTorch} \cite{Paszke2019PyTorch:Library} and \textit{PyTorch Lightning} \cite{PyTorchDocumentation}.

This fully convolutional model reduces computational requirements and learns features at different scales through its multi-resolution architecture \ref{tab:samplecnn}.

\begin{table}[h]
\centering
\small
\begin{tabularx}{\textwidth}{>{\hsize=1.6\hsize}X>{\hsize=0.6\hsize}X>{\hsize=0.6\hsize}X}
\toprule
\thead{\textbf{Layer Type}} & \thead{\textbf{In Channels}} & \thead{\textbf{Out Channels}} \\
\midrule
Conv + ReLU & 1 & 128 \\
\addlinespace
\multicolumn{3}{c}{The following layers are repeated depending on the strides and hidden parameters:} \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 128 & 128 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 128 & 128 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 128 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 256 \\
\addlinespace
Conv + BatchNorm + ReLU + MaxPool & 256 & 512 \\
\addlinespace
Conv + BatchNorm + ReLU + AvgPool & 512 & 512 \\
\addlinespace
Fully connected or representation layer & 512 & 128 \\
\bottomrule
\end{tabularx}
\caption{Layer specifications for SampleCNN model}
\label{tab:samplecnn}
\end{table}

The original model has been modified slightly to serve our specific needs: the output from the final convolutional layer is now subjected to an average pooling operation.

%Colour CNN
\input{figures/neural networks/colour CNN}

\subsection{Optimizer and learning rate}

We've employed the AdamW optimizer \cite{Loshchilov2017DecoupledRegularization}, an optimized variant of the widely-used Adam optimizer for training neural networks. AdamW adeptly balances the learning rate across network weights, providing an efficient strategy for weight decay management by isolating it from gradient updates. 

The learning rate, set at 0.003, is a pivotal parameter dictating the step size at each iteration towards loss function minimization. It's a delicate balancing act— a high rate promises swift convergence with a risk of minimum overshoot, while a lower rate provides careful convergence but necessitates more iterations. Given that delicacy, we set it to a standard number broadly used in the literature.

\subsection{Audio augmentation and transformation pipeline}

The choice of input data is guided by the task, computational resources, and the need to balance data retention with computational efficiency.

Previous research has utilized Convolutional Neural Networks (CNNs) with various features such as Mel-Scaled Log-magnitude Spectograms (MLS), Self-Similarity Matrices (SSM), and Self-Similarity Lag Matrices (SSLM) as inputs \cite{Hernandez-Olivan2021MusicFeatures}. However, features derived from raw audio may lack interpretability in some scenarios \cite{Schindler2020DeepTutorial}, and raw audio presents unique advantages despite being computationally demanding. It ensures the preservation of the original signal, potentially unearthing novel insights, and allows for direct feature extraction via advanced deep learning models \cite{learning}. Nevertheless, it comes with challenges such as high dimensionality due to high sample rates, requiring substantial computational resources. Moreover, it is sensitive to noise, adding complexity to the extraction process.

Time-domain processing naturally handles temporal patterns and sequences in data, thereby avoiding windowing artifacts. Although Mel-spectrogram-based methods are effective for various audio-related machine learning tasks, their limitations lie in representing perceptual similarity. Spectrograms capture frequency distribution over time. Yet, the complexity of human auditory perception—encompassing temporal patterns, phase relationships between frequencies, and higher-level musical structures—means that musically similar sounds can have distinct spectrograms. This discrepancy implies that using spectrogram distance alone for measuring high-level music content may not always align with human perceptions \cite{Kim2020OneStrategies, Mesostructures2023}.

\subsubsection{Positive sample generation chain}

The positive image in every triplet of input data must preserve its intelligible content when subjected to transformations, regardless of alterations in sonic qualities and artifacts. Maintaining the temporal structure and meaningfulness of the content allows it to present musical elements identical or remarkably close to the original track.

While we have experimented with helpful audio augmentation tools \cite{Spijkervet2021Spijkervet/torchaudio-augmentations:V1.0, Kharitonov2020DataDomain}, the specific requirements of our experiments necessitated the development of our transformation chain using \textit{torchaudio}'s \cite{Yang2021TorchAudio:Processing} implementation of \textit{SoX} \cite{sox}: given an anchor audio signal $A[n]$, we generate a positive signal $P[n]$ by applying a series of amplitude, time-domain, frequency-domain, modulation, reverberation, and nonlinear effects with additive noise on top of it. 

\textbf{Amplitude effects}, we use gain to modify the signal's amplitude with a constant factor $g \in [-12, 0]$.

\textbf{Time-domain effects} such as speed change and stretch alter the signal's playback speed and duration without changing its pitch, respectively. The speed and stretch factors are $\alpha \in [0.9, 1.1]$ for speed change and $\beta \in [0.9, 1.1]$ for the stretch factor.

\textbf{Frequency-domain effects} adjust the frequency content or pitch of the signal, as in pitch shifting, which alters the pitch by $\Delta p \in [-1200, 1200]$.

\textbf{Nonlinear effects} such as overdrive add harmonic distortion using the parameter $d \in [0, 30]$ for overdrive

\textbf{Modulation effects}, including chorus and tremolo, use a control signal or low-frequency oscillator. The specified ranges determine the chorus parameters and the tremolo's amplitude modulation frequency and depth $f_m \in [0.1, 100]$ for the tremolo's modulation frequency and $d_m \in [1, 101]$ for the tremolo's depth

\textbf{Reverberation effects} simulate physical space's acoustic reflections and reverberations, like reverb that applies an impulse response $h_R[n]$. 

Finally, \textbf{Noise effects} add a noise signal $N[n]$ with a signal-to-noise ratio (SNR) in the range $[12, 100]$.

The positive signal $P[n]$ is generated according to the equation given below, where the symbols denote specific effects as indicated:

\begin{equation}\label{eq:positive_signal}
P[n] = A[n] \ast h_{G}[n] \ast h_{C}[n] \ast h_{D}[n] \ast h_{P_t}[n] \ast h_{R}[n] \ast h_{S}[n] \ast h_{T}[n] \ast h_{T_m}[n] + N[n]
\end{equation}

The $\ast$ symbol represents convolution, and the various $h$ symbols indicate the impulse responses of specific effects. The noise signal $N[n]$ is included with a specific SNR.

\subsubsection{Negative sample generation}

Circling back to our premises and assumptions, we posit that high-level musical content unfolds over time; therefore, we argue that the temporal structure of the negative images, in contrast to our anchor, should be disrupted. While maintaining similar sonic qualities, the content should be rendered unintelligible.

The computation goes as follows:

\begin{enumerate}
\item We first calculate the minimum and maximum audio chunk lengths in samples:
\begin{equation}
l_{min} = t_{min} \times S
\end{equation}
\begin{equation}
l_{max} = t_{max} \times S
\end{equation}

The minimum duration $t_{min}$ is set to 0.05 seconds, and the maximum duration $t_{max}$ is set to 1 second. This range is chosen thoughtfully to strike a balance between two factors: on the one hand, it is above the just noticeable difference (JND) threshold, the smallest change in a stimulus that can be perceived. On the other hand, it is short enough to maintain a reasonably-sized window to avoid discernible musical content \cite{Fastl2007Just-NoticeableChanges}.

\item We then generate random audio chunk lengths $l_1, l_2, \ldots, l_{n-1}$ from the uniform distribution on the interval $[l_{min}, l_{max}]$. Calculate the final audio chunk length as:
\begin{equation}
l_n = L_A - \sum_{i=1}^{n-1} l_i
\end{equation}
where $L_A$ is the length of the anchor signal in samples.

\item The third step is to split the anchor signal $A$ into audio chunks $C_1, C_2, \ldots, C_n$ according to the calculated audio chunk lengths in the previous step.

\item Shuffle the audio chunks randomly to get the permuted slices $C_{\sigma(1)}, C_{\sigma(2)}, \ldots, C_{\sigma(n)}$, where $\sigma$ is a random permutation of indices from $1$ to $n$. 

\item We finally concatenate the shuffled audio chunks to generate the negative signal that will have similar production while the content is completely ruined:
\begin{equation}\label{eq:negative_signal}
N = C_{\sigma(1)} \oplus C_{\sigma(2)} \oplus \ldots \oplus C_{\sigma(n)}
\end{equation}
\end{enumerate}

The whole purpose of this process is to disturb the content unfolding in the time domain.

\subsection{Loss function}

Schroff, F., Kalenichenko, D., and Philbin, J. at Google initially proposed and used triplet loss to learn face recognition of the same person at different poses and angles. \cite{Schroff2015FaceNet:Clustering}

The triplet loss function used in Siamese-like networks guides the learning process. It seeks to minimize the distance between the anchor and positive instances while maximizing the distance between the anchor and the negative instances. A margin parameter is included in the loss function to ensure a minimum separation between positive and negative instances in the embedding space.

The triplet loss function $\mathcal{L}(\mathbf{a}, \mathbf{p}, \mathbf{n})$ aims to ensure that an anchor vector $\mathbf{a}_i$ is closer in the embedding space to a positive vector $\mathbf{p}_i$ (representing an example of the same class) than to a negative vector $\mathbf{n}_i$ (representing an example of a different class) by at least a margin $m$. It is calculated by summing the losses overall $N$ triplets in the dataset, where the equation gives the loss for each triplet:

\begin{equation}
\mathcal{L}(\mathbf{a}, \mathbf{p}, \mathbf{n}) = \sum_{i=1}^{N} \max \left(0, \left| \mathbf{a}_i - \mathbf{p}_i \right|_2^2 - \left| \mathbf{a}_i - \mathbf{n}_i \right|_2^2 + m \right)
\end{equation}

The final loss used for model training is then the average loss over a mini-batch of $N$ triplets:

\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(\mathbf{a}_i, \mathbf{p}_i, \mathbf{n}_i)
\end{equation}

As previously stated, the goal of minimizing this loss function is to learn discriminative embeddings, where similar examples are grouped closely together. In contrast, dissimilar examples are placed farther apart in the embedding space.

While some packages can be found in the MIR literature \cite{auraloss}, we wrote our own PyTorch \cite{Paszke2019PyTorch:Library} implementation for the sake of our experiments.

\subsection{Online triplet mining and batch normalization}

Online triplet mining is beneficial for managing large datasets by dynamically selecting the most informative triplets during training, focusing on each mini-batch. This strategy makes the process memory-efficient by negating the need to store all possible triplet combinations. Still, it also enhances model performance through quicker convergence by focusing on challenging examples based on the current model state.

This hard triplet mining selects triplets $(a, p, n)$ to maximize the Euclidean distance between the anchor and positive samples and the anchor and negative samples. These distances, $D_{\text{AP}}$ and $D_{\text{AN}}$, are computed respectively as:


\begin{align}
D_{\text{AP}} &= \sqrt{\sum_{i} (A_i - P_i)^2} & D_{\text{AN}} &= \sqrt{\sum_{i} (A_i - N_i)^2}
\end{align}


In implementing the batch normalization step, it is necessary to standardize the audio lengths across all elements in the minibatch. We opted to zero-pad all clips to the length of the longest clip, valuing data integrity and completeness over potential performance trade-offs. Thus, the length of the longest array in the batch, which sets the standard for all others, is as follows:

\begin{equation}
L_{\text{max}} = \max_{i \in I} \left( \max \left( |A_i|, |P_i|, |N_i| \right) \right)
\end{equation}

$I$ represents the set of all items in the batch, $|A_i|$, $|P_i|$, and $|N_i|$ denote the lengths of the anchor, positive, and negative embedding vectors for the $i$-th item, respectively. The $\max$ function is applied to find the longest of these three lengths for each item, and then the maximum of these maximum lengths is taken over all items in the batch. This gives the maximum length, $L_{\text{max}}$, of any vector in the batch.

\newpage