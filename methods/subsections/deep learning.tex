\section{Deep Learning Modeling}

Deep learning (DL), a subset of machine learning, employs deep neural networks to decipher hierarchical data representations, allowing models with multiple processing layers to abstract data across numerous levels \cite{LeCun2015DeepLearning}. DL models have significantly improved various domains due to their nonlinear modeling capabilities and scalability with large datasets.

They leverage the central algorithm, backpropagation, which calculates gradients of the loss function related to network parameters through forward and backward pass processes, thereby optimizing model performance \cite{Rumelhart1986LearningInternal}.

%Backpropagation
\input{figures/neural networks/Back_propagation}

Deep Neural Networks (DNNs), computational models rooted in biological neural systems, utilize layers of artificial neurons for tasks such as pattern recognition, classification, and regression. DNNs evolve by adjusting connections between neurons during learning, a concept initiated in the mid-20th century but modernized with Frank Rosenblatt's single-layer perceptron in 1957, capable of learning linearly separable patterns \cite{Rosenblatt1958ThePerceptron}.

%Multi-layer_NN
\input{figures/neural networks/Multi-layer_NN}

Another crucial component in the operation of DNNs is the activation function. This function is applied to a neuron's inputs, transforming the input's summed product and weights into an output passed onto the next layer. It introduces non-linearity into the network, enabling it to learn complex patterns and relationships from the data \cite{Goodfellow2016DeepLearning}.

%Activation equation
\input{figures/neural networks/activation}

\newpage