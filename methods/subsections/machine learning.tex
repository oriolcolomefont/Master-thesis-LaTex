\section{Introduction}

Throughout this chapter, we aim to provide a comprehensive understanding of the methodology employed in our research, laying the foundation for the presentation and discussion of the results in the following chapters.

\section{Artificial Neural Networks (ANNs)}
Artificial Neural Networks (ANNs) are computational models inspired by biological neural systems. They use interconnected layers of artificial neurons to process information and perform tasks like pattern recognition, classification, and regression. ANNs adjust the connections, or "weights," between neurons during learning to improve performance. The concept of neural networks emerged in the mid-20th century with the work of Warren McCulloch and Walter Pitts. Still, modern neural networks began with Frank Rosenblatt's perceptron in 1957, a single-layer network able to learn linearly separable patterns.

%Single neuron
\input{figures/neural networks/Single Neuron}

%NN
\input{figures/neural networks/Network_graph}

It consists of an input layer, one or more hidden layers, and an output layer. Each neuron in a layer receives input from the previous layer and computes an output value using an activation function. The output of the last layer represents the prediction or classification result.

%Activation equation
\input{figures/neural networks/activation}

%Multi-layer_NN
\input{figures/neural networks/Multi-layer_NN}

\section{Deep Learning (DL)}


Deep learning is a subfield of machine learning that employs deep neural networks to learn hierarchical data representations, enabling computational models with multiple processing layers to learn data abstractions across numerous levels. These methods have dramatically advanced the state-of-the-art in various domains, including speech recognition, visual object recognition, object detection, drug discovery, genomics, and more \cite{LeCun2015DeepLearning}. The effectiveness of deep learning arises from its nonlinear modeling capabilities and scalability with large datasets.

A central algorithm in deep learning is backpropagation, which uses the forward and backward pass process to efficiently compute gradients of the loss function concerning the network's parameters. The forward pass involves propagating input data through the network to generate predictions. In contrast, the backward pass calculates the error between predictions and ground truth and subsequently returns it through the network. This procedure updates the network's parameters using gradient descent, minimizing the loss function and enhancing the model's performance.

Deep learning has led to breakthroughs in processing complex, high-dimensional data, including images, video, speech, and audio, primarily due to deep convolutional nets. Simultaneously, recurrent nets have illuminated sequential data analysis such as text and speech. Deep learning discovers intricate structures in large datasets, improving our understanding and processing capabilities across various applications.

%Backpropagation
\input{figures/neural networks/Back_propagation}

\newpage


