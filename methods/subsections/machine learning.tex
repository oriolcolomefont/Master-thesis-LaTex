\section{Introduction}
Machine Learning (ML), particularly Deep Learning, has significantly advanced the MIR field by enabling automatic feature learning, improving classification and analysis tasks, and facilitating novel applications like music generation and source separation. Adopting musically motivated neural network architectures \cite{musicmotivCNN} blends musical concepts and perceptual approaches and use this knowledge to analyze and understand music in a way that goes beyond traditional music analysis techniques. 

\section{Artificial Neural Networks (ANNs)}
A neural network, formally known as an artificial neural network (ANN), is a computational model inspired by the structure and functionality of biological neural networks. It is a mathematical framework designed to process information and perform tasks like pattern recognition, classification, and regression by mimicking how biological neurons and synapses operate. ANNs comprise interconnected layers of artificial neurons or nodes, each performing a mathematical transformation on the input data to generate an output. The connections between these neurons, often called "weights," are adjusted during learning to optimize the network's performance on a given task.

\subsection{About ANNs}
The concept of neural networks can be traced back to the early 20th century, with the pioneering work of Warren McCulloch and Walter Pitts in 1943. They proposed a mathematical model of a neuron known as the McCulloch-Pitts neuron, which provided a foundation for future research in artificial intelligence. However, Frank Rosenblatt's invention of the perceptron in 1957 marked the beginning of modern neural networks. The perceptron is a single-layer neural network capable of learning linearly separable patterns.

%Single neuron
\input{figures/neural networks/Single Neuron}

%NN
\input{figures/neural networks/Network_graph}

In a more technical description, an ANN consists of an input layer, one or more hidden layers, and an output layer. Each neuron in a layer receives input from the previous layer and computes an output value using an activation function. Some standard activation functions include the sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU). The output of the last layer represents the prediction or classification result.

%Activation equation
\input{figures/neural networks/activation}

%Multi-layer_NN
\input{figures/neural networks/Multi-layer_NN}

\section{Deep Learning (DL)}

Deep learning is a machine learning subfield that utilizes deep neural networks to learn hierarchical data representations automatically. These networks handle complex, high-dimensional data across domains such as computer vision and natural language processing. Their effectiveness stems from their nonlinear modeling capabilities and scalability with large datasets.

A core algorithm in deep learning is backpropagation which efficiently computes gradients of the loss function concerning the network's parameters. It involves a two-step process: (1) forward pass, where input data propagates through the network to generate predictions, and (2) backward pass, where the error between predictions and ground truth is calculated and propagated back through the network. This process updates the network's parameters using gradient descent, which minimizes the loss function and improves the model's performance.

%Backpropagation
\input{figures/neural networks/Back_propagation}

\subsection{Convolutional Neural Network (CNN)}

A Convolutional Neural Network (CNN) is a deep-learning neural network. The architecture of a CNN is composed of several layers, including:

\begin{itemize}

\item \textbf{Convolutional layers:} These layers apply a convolution operation to the input data. The convolutional layers are designed to effectively learn local patterns and features in the audio data \footnote{For a visual explanation and further understanding of the operation convolution, see this video: \url{https://youtu.be/KuXjwB4LzSA}}.

A convolution operation involves sliding a small window, called a filter or kernel, over input data to compute the dot product between the kernel and each local region of the data. The result is a new feature map highlighting patterns and essential information.

Given a 2D input data represented as a matrix $I = [i_{ij}]{m \times n}$ and a filter $K = [k{ij}]_{p \times q}$, the convolution operation is defined as:

$$(I * K)_{kl} = \sum_{i=1}^{p} \sum_{j=1}^{q} i_{k+i-1, l+j-1}k_{ij}$$

Here, $k$ ranges from $1$ to $m-p+1$ and $l$ ranges from $1$ to $n-q+1$. The resulting feature map after applying the convolution is denoted by $(I * K)_{kl}$.

The convolution operation slides the kernel $K$ over the input image $I$, performing an element-wise multiplication and sum of the local region centered at pixel $(k,l)$ to compute the value of each element in the output feature map, $(I * K)_{kl}$. This process is repeated for each pixel $(k,l)$, resulting in a new feature map that captures local patterns in the input image.

\input{figures/neural networks/convolution}
%Single CNN
\input{figures/neural networks/Single_CNN}

\item \textbf{Pooling layers:} These layers downsample the data, reducing the spatial or temporal dimensions of the input while retaining important information. A joint pooling operation is the max-pooling, which can be defined as follows: given an input matrix $I$ and a pooling window of size $m \times n$, the max-pooling operation selects the maximum value within each non-overlapping $m \times n$ region of the input matrix. This operation effectively reduces the input's dimensions while retaining the essential information in each area.
\vspace*{3mm}

%Single pooling layer
\input{figures/neural networks/Single_Pooling_Layer}

\item \textbf{Fully connected layers:} These layers connect every neuron in one layer to every neuron in another, allowing the network to learn non-linear combinations of the features discovered in the previous layers. Mathematically, a fully connected layer can be represented as $Y = XW + b$, where $X$ is the input matrix, $W$ is the weight matrix, $b$ is the bias vector, and $Y$ is the output matrix.
\vspace*{3mm}

\item \textbf{Output layer:} The output layer produces the final predictions of the network.

\end{itemize}

Compared to a traditional neural network, CNNs are more computationally efficient and have less number of parameters to train. This makes them more feasible for large-scale datasets and real-world problems.

On top of that, one of the main advantages of using CNNs for audio analysis is that they can automatically and adaptively learn temporal hierarchies of features from audio data, which traditional audio processing methods may not do effectively. 

%Colour CNN
\input{figures/neural networks/colour CNN}

\newpage


