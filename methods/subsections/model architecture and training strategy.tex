\section{Model architecture and training strategy}
\section{Self-Supervised Learning (SSL)}
Self-Supervised Learning (SSL) is a technique that leverages inductive bias to learn latent representations from large-scale data without relying on human annotations. By focusing on pretext tasks that create different, highly correlated views of an object, SSL models exploit the underlying structure and known variance in the data. 

An SSL model is trained to generalize objects' representations in a latent high-dimensional space. To achieve this, the model contrasts the representations of the same object with other objects (defined as negative samples) during training. By doing so, a contrastive SSL model is expected to produce more distinctive representations \cite{audioselfsupsurvey}. Several MIR fields, such as music genre classification, or music similarity (both in the symbolic \cite{musicviaselfsup} and time domain \cite{epidemic}), have taken advantage of SSL techniques to improve their performance and alleviate the need for large amounts of labeled data \cite{audioselfsupsurvey}.

\subsubsection{Strengths}

\begin{enumerate}
    \item Reduces dependency on labeled data: SSL capitalizes on inductive bias to learn from large amounts of unlabeled data, which is particularly beneficial when obtaining labeled data is expensive or time-consuming.
    \item Encourages feature extraction: SSL encourages learning useful features or representations from the data, which can lead to better generalization and improved performance on downstream tasks.
    \item Transfer learning: SSL models can be fine-tuned for specific tasks with smaller labeled datasets, allowing for more efficient training and potentially better results.
    \item Robustness: SSL models can learn robust features from noisy or incomplete data, improving their ability to handle real-world scenarios.
\end{enumerate}

\subsubsection{Flaws}

\begin{enumerate}
    \item Less interpretable: SSL models tend to be less interpretable than supervised models, as their learning mechanisms are less explicit and rely more on inductive bias.
    \item Computational complexity: SSL models often require more training iterations and larger batch sizes, which can result in higher computational costs.
    \item Sensitive to hyperparameters: SSL models can be sensitive to hyperparameters such as learning rate, batch size, and the choice of the contrastive loss function, which may require extensive tuning.
    \item Limited applicability: SSL may not be suitable for all tasks or domains, as its performance depends on the quality and relevance of the self-supervised pretext tasks and the effectiveness of the inductive bias in capturing the known variance in the data.
\end{enumerate} 

\subsection{Contrastive Learning of Musical Representations (CLMR)}

Contrastive Learning of Musical Representations (CLMR) is a self-supervised learning technique that aims to learn discriminative representations of music by leveraging its structure and content without explicit labels \cite{CLMR2021}\cite{multisourceclmr2023}. The learning process involves creating different augmentations or transformations of the same musical piece as positive pairs and using other randomly selected pieces as antagonistic pairs. A neural network is then trained to minimize a contrastive loss function, which encourages the model to produce similar representations for positive pairs and dissimilar representations for opposing pairs. Once trained, these learned representations can be utilized for various MIR downstream tasks.

\section{}

We propose using a Triple Siamese Network to train a model for music similarity retrieval \cite{contentmusicsimtriplet2020}. Specifically, we aim to minimize the loss function between the anchor, positive, and negative samples by using online triplet mining.

The positive samples are created by transforming and augmenting the original samples while preserving their perceptual underlying musical language. This is achieved using techniques such as time-stretching, pitch-shifting, and adding noise to the samples. By doing so, we aim to enhance the diversity of the positive samples and improve the model's ability to retrieve similar-sounding samples that share the same underlying musical language.

On the other hand, the negative samples are created by shuffling and scrambling the original samples while maintaining their production texture. This is achieved using random segment selection, time-domain filtering, and frequency-domain masking. The goal is to maintain the production texture of the samples while disrupting their underlying composition.

Using the proposed triple siamese network and online triplet mining, we aim to train a model that can effectively distinguish between similar and dissimilar samples while considering their underlying composition and production texture.

 Implements the triplet loss function, commonly used for learning embeddings in metric learning problems. The triplet loss aims to ensure that the distance between an anchor and a positive sample (both from the same class) is smaller than the distance between the anchor and a negative sample (from a different class) by a margin.

$\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N} \max \left( d\left(a_i, p_i\right) - d\left(a_i, n_i\right) + m, 0 \right)$

In this equation:

$N$ is the number of triplets in the batch
$a_i$ is the $i$-th anchor
$p_i$ is the $i$-th positive sample
$n_i$ is the $i$-th negative sample
$d(\cdot, \cdot)$ is the Euclidean distance function
$m$ is the margin; in this case, it is set to 0.2 by default
The forward method computes the triplet loss as follows:

Calculate the pairwise Euclidean distance between the anchor and positive samples, denoted as $d(a_i, p_i)$.
Calculate the pairwise Euclidean distance between the anchor and negative samples, denoted as $d(a_i, n_i)$.
Compute the loss for each triplet in the batch by subtracting the negative distance from the positive distance and adding the margin: $d(a_i, p_i) - d(a_i, n_i) + m$.
Apply the ReLU function, which is equivalent to taking the maximum value between the computed value and 0: $\max \left( d(a_i, p_i) - d(a_i, n_i) + m, 0 \right)$.
Calculate the mean of the losses across all triplets in the batch.