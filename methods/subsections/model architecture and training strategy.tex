\section{Machine Learning Modeling}

This section delves into the systematic process of developing, training and evaluating algorithms, which empower data-driven learning, predictions, and decisions. This process encompasses selecting an appropriate algorithm, designing the model architecture, preprocessing and transforming data, and extracting relevant features. By iteratively adjusting the model's parameters, we aim to minimize the prediction error. The choice of techniques, such as supervised, unsupervised, or reinforcement learning, is dictated by the specific problem and data structure. Model evaluation, fine-tuning, and deployment are essential to ensure the model generalizes well to new data and performs effectively in real-world applications.

\subsection{Model architecture and training strategy}

I advocate for exposure as the primary method of learning music, emphasizing the importance of immersing oneself in various musical experiences to foster a holistic understanding of music as an art form. Individuals can cultivate well-rounded musical expertise by engaging with different styles, genres, techniques, and learning approaches, ultimately promoting creativity and experimentation.

Given the nature of our task, we propose using a Triple Siamese Network. This model architecture has been proven efficient for music similarity retrieval tasks \cite{contentmusicsimtriplet2020}. Specifically, we aim to minimize the loss function between an anchor, a positive, and a negative sample by using online triplet mining \cite{Sikaroudi2020OfflinePatches}.

Using the proposed triple siamese network and online triplet mining, we aim to train a model that can effectively distinguish between similar and dissimilar samples while considering their underlying composition and production texture.

\subsection{Self-Supervised Learning (SSL)}

Self-supervised learning (SSL) is a subfield of machine learning where models learn to generate representations of data by finding patterns and structures within the data without relying on external labels or annotations. In other words, the model learns by creating its supervision signal from the raw, unlabeled data.\cite{audioselfsupsurvey}

This approach to learning is inspired by the way humans and animals learn from their environment, where a great deal of knowledge is acquired through observation and interaction without explicit instruction. Self-supervised learning is a machine learning process where the model trains itself to learn one part of the input from another part of the input. In this process, the unsupervised problem is transformed into a supervised problem by auto-generating the labels.

Some of the main advantages of SSL include:

\begin{enumerate}
\item Reduces dependency on labeled data, which can be expensive and time-consuming to collect and annotate.
\item Encourages more generalizable and robust representations of the data since they capture the inherent structure and properties of the data rather than relying on human-provided labels.
\end{enumerate}

Further understanding of the cognitive processes in music can be obtained through Jean Piaget's theory of cognitive development. Piaget suggests that children acquire knowledge through sensory and motor experiences in their early stages, with reasoning gradually evolving toward abstract ideas and deductive logic through the development of schemasâ€”cognitive structures that underlie human knowledge and skill \cite{Huitt2003PiagetsDevelopment}. These schemas evolve through an equilibration mechanism that incorporates new information with pre-existing knowledge, involving processes of assimilation and accommodation \cite{audioselfsupsurvey}.

\subsubsection{Known and unknown invariance}

In pattern recognition, \textit{invariance} refers to the property of a model to be robust to certain transformations or changes in the input data. In other words, an invariant model should produce the same or similar output for different instances of the same object, even if they have undergone some changes.



\begin{definition}[Known invariance]
Known invariances are transformations or changes that the model or algorithm is explicitly designed to be robust against. By incorporating these invariances into the model's design, the algorithm is better equipped to generalize and recognize patterns in the presence of these transformations.
\end{definition}

\begin{definition}[Unknown invariance]

Unknown invariances are transformations the model has not been explicitly designed to handle. However, it still manages to be robust due to its learning capabilities. These invariances can emerge from the data and may not be known a priori. 
\end{definition}

\subsection{Contrastive Learning of Musical Representations (CLMR)}

Contrastive Learning of Musical Representations (CLMR) \cite{CLMR2021} aims to learn discriminative and valuable representations of music without relying on explicit labels. By leveraging the structure and content of music and contrasting different augmentations or transformations of the same musical piece against randomly generated pieces, CLMR models can effectively capture the underlying known invariances that are explicitly incorporated into the design of a model to improve its robustness and generalization.

In contrast, the model learns unknown invariances from the data without explicit instruction.

The CLMR methodology involves the following steps:

\begin{enumerate}
\item \textbf{Data Augmentation:} Create different augmentations or transformations of the same musical piece as positive pairs and use other randomly selected pieces as antagonistic pairs.
\item \textbf{Contrastive Learning:} Train by minimizing a contrastive loss function, encouraging the model to produce similar representations for positive pairs and dissimilar representations for antagonistic pairs.
\item\textbf{Latent Representation:} Utilize the learned representations for various Music Information Retrieval (MIR) downstream tasks, such as music genre classification or similarity, both in the symbolic and time domains.
\end{enumerate}

By following these steps, CLMR models can learn meaningful and robust representations of music, potentially enhancing their performance in various MIR tasks and reducing the need for labeled data.

\subsection{Siamese Networks}

A Siamese Network, a deep learning architecture introduced by Bromley and LeCun in the early 1990s \cite{Bromley1993SignatureNetwork}, is designed explicitly for tasks involving similarity or comparison between two input instances. Its name derives from the structure, which consists of two or more identical subnetworks connected in parallel and joined at the output layer. These subnetworks share the same architecture, weights, and hyperparameters, allowing for more efficient memory usage and computational complexity. 

This design is particularly effective for learning from limited or imbalanced datasets, as it focuses on learning the similarity metric rather than specific features of individual classes.

Each subnetwork processes an input independently, and its outputs are combined and further processed to yield a single similarity score. The shared weights during training enable the model to learn an invariant representation for the input instances, improving its efficiency in comparing and contrasting them. Siamese Networks employ specialized loss functions, such as contrastive or triplet loss, which minimize the distance between similar input pairs and maximize the distance between dissimilar ones. This approach encourages the network to learn a meaningful similarity metric.


\subsubsection{Triplet Siamese Networks}

Triplet Siamese Networks, an extension of the Siamese Network architecture, involve comparing three input instances instead of two, aiming to learn an embedding space where similar instances are close and dissimilar instances are distant. The input consists of an anchor, a positive instance (same class as the anchor), and a negative instance (different class). An identical subnetwork with shared architecture, weights, and hyperparameters processes each instance.

The triplet loss function guides learning, minimizing the distance between the anchor and positive instances while maximizing the distance between the anchor and negative instances. A margin parameter in the loss function ensures a minimum separation between positive and negative instances in the embedding space.

Triplet Siamese Networks offer several advantages, including learning fine-grained similarity metrics, being well-suited for one-shot learning tasks, and offering efficient memory usage and computational complexity due to shared weights. These networks have been applied to various tasks and domains, such as image recognition, face recognition, speaker recognition, text similarity, and medical imaging for diagnosis or treatment purposes.


\subsection{Convolutional Neural Network (CNN)}

A Convolutional Neural Network (CNN) is a deep-learning neural network. The architecture of a CNN is composed of several layers, including:

\begin{itemize}

\item \textbf{Convolutional layers:} These layers apply a convolution operation to the input data. The convolutional layers are designed to effectively learn local patterns and features in the audio data \footnote{For a visual explanation and further understanding of the convolution operation, please refer to this URL: \url{https://youtu.be/KuXjwB4LzSA}}.

Let's consider a 1D input sequence or signal represented as a vector $I = [i_{i}]{n}$ and a filter $K = [k_{j}]_{p}$, where $n$ is the length of the input sequence and $p$ is the length of the filter. The convolution operation is defined as:

\begin{equation}
(I * K){k} = \sum{j=1}^{p} i_{k+j-1}k_{j}
\end{equation}

Here, $k$ ranges from $1$ to $n-p+1$. The resulting feature vector after applying the convolution is denoted by $(I * K)_{k}$.

The convolution operation slides the kernel $K$ over the input sequence $I$, performing an element-wise multiplication and sum of the local region centered at element $k$ to compute the value of each element in the output feature vector, $(I * K)_{k}$. This process is repeated for each component $k$, resulting in a new feature vector that captures local patterns in the input sequence.

In simple terms, the filter or kernel is slid across the 1D input data. For each position, we perform an element-wise multiplication of the filter with the portion of the input it currently covers and sums the results. This value becomes the corresponding entry in the output. The process is repeated for each position the filter can occupy.

\item \textbf{Pooling layers:} These layers downsample the data, reducing the input dimensions while retaining important information. A joint pooling operation is the max-pooling, which can be defined as follows: given an input matrix $I$ and a pooling window of size $m \times n$, the max-pooling operation selects the maximum value within each non-overlapping $m \times n$ region of the input matrix. This operation effectively reduces the input's dimensions while retaining the essential information in each area.
\vspace*{3mm}

\item \textbf{Fully connected layers:} These layers connect every neuron in one layer to every neuron in another, allowing the network to learn non-linear combinations of the features discovered in the previous layers. Mathematically, a fully connected layer can be represented as $Y = XW + b$, where $X$ is the input matrix, $W$ is the weight matrix, $b$ is the bias vector, and $Y$ is the output matrix.
\vspace*{3mm}

\item \textbf{Output layer:} The output layer produces the final predictions of the network.

\end{itemize}

Compared to a traditional neural network, CNNs are more computationally efficient and have less number of parameters to train. This makes them more feasible for large-scale datasets and real-world problems. On top of that, one of the main advantages of using CNNs for audio analysis is that they can automatically and adaptively learn temporal hierarchies of features from audio data, which traditional audio processing methods may not do effectively. 

%Colour CNN
\input{figures/neural networks/colour CNN}


\subsection{Encoder: SampleCNN}

The SampleCNN model \cite{Lee2018SampleCNN:Classification} takes in 1D input data with a single channel. The first layer is a 1D convolutional layer with a kernel size of 3, a stride of 3, and 128 output channels. It is followed by batch normalization and ReLU activation.

After the initial layer, there are nine hidden layers with varying kernel sizes, strides, and channels. Each hidden layer consists of the following:

\begin{itemize}
    \item A 1D convolutional layer with the specified number of input and output channels, kernel size, a stride of 1, and padding of 1.

    \begin{equation}
y_{c_o}(n) = \sum_{c_i=1}^{C_{\text{in}}} \sum_{k=-\lfloor K/2 \rfloor}^{\lfloor K/2 \rfloor} x_{c_i}(n - k) \cdot w_{c_o, c_i}(k)
\end{equation}

Where:

$y_{c_o}(n)$ is the output for the $c_o$-th output channel at position $n$, $C_{\text{in}}$ is the number of input channels, $x_{c_i}(n)$ is the input for the $c_i$-th input channel at position $n$, $w_{c_o, c_i}(k)$ is the kernel weight for the $c_o$-th output channel and $c_i$-th input, channel at position $k$, and $K$ is the kernel size. The stride is set to 1 and padding to 1 (equal padding $\lfloor K/2 \rfloor$ on both sides).

\item Batch normalization. We process a batch of data containing anchor, positive, and negative samples. We pad the waveforms to the same  maximum length among the collection, perform online triplet mining to find the hardest negatives and return the padded anchors, positives, and hardest negatives.

We have intentionally chosen to use zero padding instead of array truncating in our experiment. This decision was based on our belief that preserving the integrity of the music data was more critical, despite the potential trade-off with computational efficiency during the normalization process.

Let $A$, $P$, and $N$ represent the anchor, positive, and negative samples. The maximum array length in the batch is given by:

\begin{equation}
L_{\text{max}} = \max_{\text{item} \in \text{batch}}(\max(\text{length}(A_{\text{item}}), \text{length}(P_{\text{item}}), \text{length}(N_{\text{item}})))
\end{equation}

The anchor-positive distance and the anchor-negative distance are computed as the Euclidean distance between the corresponding samples:

\begin{equation}
D_{\text{AP}} = \sqrt{\sum_{i} (A_i - P_i)^2}
\end{equation}

The "hardest" negative index for each anchor-positive pair is determined by maximizing the difference between the anchor-negative and anchor-positive distances:

\begin{equation}
D_{\text{AN}} = \sqrt{\sum_{i} (A_i - N_i)^2}
\end{equation}

    
\item ReLU activation. The Rectified Linear Unit (ReLU) activation function is a simple yet highly effective non-linear function used in neural networks. It helps introduce non-linearity to the model and accelerates training due to its computational efficiency.

    
\item Max pooling with kernel size and stride equal to the stride value of the current layer.
\end{itemize}

After the hidden layers, there's another 1D convolutional layer with 512 input channels, 512 output channels, a kernel size of 3, a stride of 1, and padding of 1. This is followed by batch normalization and ReLU activation.

The original model has been modified slightly to serve our specific needs: the output from the final convolutional layer is now subjected to an average pooling operation across the temporal dimension.

This adaptation of the original architecture incorporates an average pooling step at the end.

\begin{figure}[ht]
\centering
\begin{equation}
y_i = \frac{1}{N} \sum_{j=1}^{N} x_{ij}
\end{equation}
\caption[Average Pooling operation]{\small{Representation of the Average Pooling operation. Here, $y_i$ is the output tensor after the average pooling process. Each element $x_{ij}$ is part of the 2D input tensor. $N$ is the count of elements in the second dimension of the input tensor, while $i$ and $j$ are used to iterate over the first and second dimensions of the tensors, respectively.}}
\end{figure}


If the model is "supervised", dropout with a rate of 0.5 is applied; a regularization technique randomly drops out neurons during training to prevent overfitting, with a 0.5 rate meaning half of the neurons are dropped out.

\input{figures/neural networks/dropout}

Finally, a linear layer with 512 input features and 128 output dimension features is applied to generate the embeddings.

\input{figures/my model/SampleCNN}

\subsection{Audio augmentation and transformation pipeline}

\subsubsection{Positive sample generation}
Given an anchor audio signal $A[n]$, we generate a positive signal $P[n]$ by applying a series of amplitude, time-domain, frequency-domain, modulation, reverberation, and nonlinear effects with additive noise on top of the chain. 

Let $g \in [-12, 0]$ represent the gain, $\Delta p \in [-1200, 1200]$ the pitch shift, $h_R[n]$ the impulse response of the reverb with parameters in $[0, 100]$, $h_C[n]$ the impulse response of the chorus with parameters determined by the specified ranges, $d \in [0, 30]$ the overdrive parameter, $\alpha \in [0.9, 1.1]$ the speed change factor, $\beta \in [0.9, 1.1]$ the stretch factor, $f_m \in [0.1, 100]$ the modulation frequency of the tremolo, and $d_m \in [1, 101]$ the depth of the tremolo. 

The positive signal $P[n]$ is generated as follows:

\begin{equation}\label{eq:positive_signal}
P[n] = A[n] \ast h_{G}[n] \ast h_{C}[n] \ast h_{D}[n] \ast h_{P_t}[n] \ast h_{R}[n] \ast h_{S}[n] \ast h_{T}[n] \ast h_{T_m}[n] + N[n]
\end{equation}

where $\ast$ denotes convolution, $h_{G}[n] = g \delta[n]$, $h_{D}[n]$ is the impulse response of the overdrive effect, $h_{P_t}[n]$ is the impulse response of the pitch shift effect, $h_{S}[n]$ represents the impulse response of the speed change effect, $h_{T}[n]$ is the impulse response of the stretch effect, and $h_{T_m}[n] = (1 - d_m \cos(2 \pi f_m n))\delta[n]$ is the impulse response of the tremolo effect. The noise signal $N[n]$ is added with a specific signal-to-noise ratio (SNR) in the range $[12, 100]$ in decibels.

These effect selections behave and affect the signal in several domains, adding aggressive transformations while preserving high-level musical content.

\begin{itemize}
\item \textbf{Amplitude effects:} Amplitude or gain modification.
\begin{itemize}
    \item Gain: Adjusts the overall amplitude of the signal by a constant factor $g \in [-12, 0]$ dB.
\end{itemize}

\item \textbf{Time-domain effects:} Alters the signal's timing or duration.
\begin{itemize}
    \item Speed change: Alters the playback speed of the signal by a factor $\alpha \in [0.9, 1.1]$.
    \item Stretch: Changes the signal duration without affecting its pitch by a factor $\beta \in [0.9, 1.1]$.
\end{itemize}

\item \textbf{Frequency-domain effects:} Alters the frequency content or pitch of the signal.
\begin{itemize}
    \item Pitch shift: Changes the signal's pitch by $\Delta p \in [-1200, 1200]$ cents. Two-octave range, one octave higher to one octave lower.
\end{itemize}

\item \textbf{Nonlinear effects:} Harmonic distortion or other complex changes.
\begin{itemize}
    \item Overdrive: Adds harmonic distortion to the signal based on the parameter $d \in [0, 30]$.
\end{itemize}

\item \textbf{Modulation effects:} Modulation using a low-frequency oscillator or control signal.
\begin{itemize}
    \item Chorus: Applies a varying time delay to the signal, resulting in a richer, thicker sound. The specified ranges determine the chorus parameters.
    \item Tremolo: Modulates the signal's amplitude at a modulation frequency $f_m \in [0.1, 100]$ Hz and depth $d_m \in [1, 101]$.
\end{itemize}

\item \textbf{Reverberation effects:} Acoustic reflections simulation and reverberations of physical space.
\begin{itemize}
    \item Reverb: Applies an impulse response $h_R[n]$ that simulates the reverberation of space with parameters in the range $[0, 100]$.
\end{itemize}

\item \textbf{Noise effects:}
\begin{itemize}
    \item Additive noise: Adds a noise signal $N[n]$ with a specific signal-to-noise ratio (SNR) in the range $[12, 100]$.
\end{itemize}
\end{itemize}

\subsubsection{Negative sample generation}

\begin{enumerate}
\item Calculate the minimum and maximum audio chunk lengths in samples:
2\begin{equation}
l_{min} = t_{min} \times S
\end{equation}
\begin{equation}
l_{max} = t_{max} \times S
\end{equation}
\item Generate random audio chunk lengths $l_1, l_2, \ldots, l_{n-1}$ from the uniform distribution on the interval $[l_{min}, l_{max}]$. Calculate the final audio chunk length as:
\begin{equation}
l_n = L_A - \sum_{i=1}^{n-1} l_i
\end{equation}
where $L_A$ is the length of the anchor signal in samples.
\item Split the anchor signal $A$ into audio chunks $C_1, C_2, \ldots, C_n$ according to the calculated audio chunk lengths.
\item Shuffle the audio chunks randomly to get the permuted slices $C_{\sigma(1)}, C_{\sigma(2)}, \ldots, C_{\sigma(n)}$, where $\sigma$ is a random permutation of indices from $1$ to $n$.
\item Concatenate the shuffled audio chunks to generate the negative signal:
\begin{equation}\label{eq:negative_signal}
N = C_{\sigma(1)} \oplus C_{\sigma(2)} \oplus \ldots \oplus C_{\sigma(n)}
\end{equation}
\end{enumerate}

\subsection{Loss function}

Schroff, F., Kalenichenko, D., and Philbin, J. at Google initially proposed and used triplet loss to learn face recognition of the same person at different poses and angles. \cite{Schroff2015FaceNet:Clustering}

\begin{equation}
\mathcal{L}(\mathbf{a}, \mathbf{p}, \mathbf{n}) = \sum_{i=1}^{N} \max \left(0, \left| \mathbf{a}_i - \mathbf{p}_i \right|_2^2 - \left| \mathbf{a}_i - \mathbf{n}_i \right|_2^2 + m \right)
\end{equation}

In this equation, triplet loss function with a variable margin:

$\mathcal{L}$ is the triplet loss function.
$\mathbf{a}_i$, $\mathbf{p}_i$, and $\mathbf{n}_i$ are the anchor, positive, and negative embedding vectors, respectively, for the $i$-th sample.
$\left| \cdot \right|_2^2$ denotes the squared Euclidean distance between two points.
$m$ is the margin, a hyperparameter that helps ensure that the anchor-positive distance is smaller than the anchor-negative distance by at least a margin.
The summation $\sum_{i=1}^{N}$ is over all $N$ triplets in the dataset.
The goal of the triplet loss is to minimize the distance between the anchor and the positive sample while maximizing the distance between the anchor and the negative sample. This encourages the model to learn embeddings that are similar for the same class and dissimilar for different classes.

The final form of the loss function is obtained by averaging the loss over a mini-batch of triplets. 

\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} L(anchor_i, positive_i, negative_i)
\end{equation}

Here, $N$ is the number of triplets in the mini-batch, and $L(anchor_i, positive_i, negative_i)$ represents the loss function computed for the i-th triplet in the batch.