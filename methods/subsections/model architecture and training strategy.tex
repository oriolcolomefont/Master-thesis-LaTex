\section{Model architecture and training strategy}
\subsection{Contrastive Self-Supervised Learning for Musical Representations (CSSLMR)}

Contrastive Self-Supervised Learning for Musical Representations (CSSLMR) is an approach that combines the principles of Self-Supervised Learning (SSL) and Contrastive Learning of Musical Representations (CLMR) to learn discriminative and valuable representations of music without relying on explicit labels. By leveraging the structure and content of music and contrasting different augmentations or transformations of the same musical piece against randomly (or not) selected pieces, CSSLMR models can effectively capture the underlying structure and known invariance in the data.

The CSSL methodology involves the following steps:

\begin{enumerate}
\item \textbf{Data Augmentation:} Create different augmentations or transformations of the same musical piece as positive pairs and use other randomly selected pieces as antagonistic pairs.
\item \textbf{Contrastive Learning:} Train a neural network to minimize a contrastive loss function, which encourages the model to produce similar representations for positive pairs and dissimilar representations for antagonistic pairs.
\item\textbf{Latent Representation:} Utilize the learned representations for various MIR downstream tasks, such as music genre classification or music similarity, both in the symbolic and time domains.
\end{enumerate} 

\subsubsection{Strengths}

CSSLMR inherits the strengths of both SSL and CLMR:

\begin{enumerate}
\item Reduces dependency on labeled data: by leveraging inductive bias (assuming a known invariance across the data samples), CSSL can learn from large amounts of unlabeled data.
\item Encourages feature extraction: CSSLMR promotes learning useful and discriminative features or representations
\end{enumerate} 

\section{Model architecture}

We propose using a Triple Siamese Network to train a model for music similarity retrieval \cite{contentmusicsimtriplet2020}. Specifically, we aim to minimize the loss function between the anchor, positive, and negative samples by using online triplet mining \cite{Sikaroudi2020OfflinePatches}.

The positive samples are created by transforming and augmenting the original samples while preserving their perceptual underlying musical language. This is achieved using techniques such as time-stretching, pitch-shifting, and adding noise to the samples. By doing so, we aim to enhance the diversity of the positive samples and improve the model's ability to retrieve similar-sounding samples that share the same underlying musical language.

On the other hand, the negative samples are created by shuffling and scrambling the original samples while maintaining their production texture. This is achieved using random segment selection, time-domain filtering, and frequency-domain masking. The goal is to maintain the production texture of the samples while disrupting their underlying composition.

Using the proposed triple siamese network and online triplet mining, we aim to train a model that can effectively distinguish between similar and dissimilar samples while considering their underlying composition and production texture.

 Implements the triplet loss function, commonly used for learning embeddings in metric learning problems. The triplet loss aims to ensure that the distance between an anchor and a positive sample (both from the same class) is smaller than the distance between the anchor and a negative sample (from a different class) by a margin.

$\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N} \max \left( d\left(a_i, p_i\right) - d\left(a_i, n_i\right) + m, 0 \right)$

In this equation:

$N$ is the number of triplets in the batch
$a_i$ is the $i$-th anchor
$p_i$ is the $i$-th positive sample
$n_i$ is the $i$-th negative sample
$d(\cdot, \cdot)$ is the Euclidean distance function
$m$ is the margin; in this case, it is set to 0.2 by default
The forward method computes the triplet loss as follows:

Calculate the pairwise Euclidean distance between the anchor and positive samples, denoted as $d(a_i, p_i)$.
Calculate the pairwise Euclidean distance between the anchor and negative samples, denoted as $d(a_i, n_i)$.
Compute the loss for each triplet in the batch by subtracting the negative distance from the positive distance and adding the margin: $d(a_i, p_i) - d(a_i, n_i) + m$.
Apply the ReLU function, which is equivalent to taking the maximum value between the computed value and 0: $\max \left( d(a_i, p_i) - d(a_i, n_i) + m, 0 \right)$.
Calculate the mean of the losses across all triplets in the batch.