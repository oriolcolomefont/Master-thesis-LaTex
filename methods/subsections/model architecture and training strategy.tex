\section{Machine Learning Modeling}

This section delves into the systematic process of developing, training and evaluating algorithms, which empower data-driven learning, predictions, and decisions. This process encompasses selecting an appropriate algorithm, designing the model architecture, preprocessing and transforming data, and extracting relevant features. By iteratively adjusting the model's parameters, we aim to minimize the prediction error. The choice of techniques, such as supervised, unsupervised, or reinforcement learning, is dictated by the specific problem and data structure. Model evaluation, fine-tuning, and deployment are essential to ensure the model generalizes well to new data and performs effectively in real-world applications.


\subsection{Model architecture and training strategy}

Given the nature of our task, we propose using a Triple Siamese Network. This model architecture has been proven efficient for music similarity retrieval tasks \cite{contentmusicsimtriplet2020}. Specifically, we aim to minimize the loss function between an anchor, a positive, and a negative sample by using online triplet mining \cite{Sikaroudi2020OfflinePatches}.

Using the proposed triple siamese network and online triplet mining, we aim to train a model that can effectively distinguish between similar and dissimilar samples while considering their underlying composition and production texture.

\subsection{Problem to solve}

\subsection{Self-Supervised Learning (SSL)}

Self-supervised learning is a subfield of machine learning where models learn to generate representations of data by finding patterns and structures within the data without relying on external labels or annotations. In other words, the model learns by creating its supervision signal from the raw, unlabeled data.\cite{audioselfsupsurvey}

This approach to learning is inspired by the way humans and animals learn from their environment, where a great deal of knowledge is acquired through observation and interaction without explicit instruction. Self-supervised learning algorithms often involve tasks where the model learns to predict or reconstruct parts of the input data, such as predicting the next word in a sentence or completing an image with missing pixels.

Some of the main advantages of self-supervised learning include:

\begin{enumerate}
\item Reduces dependency on labeled data, which can be expensive and time-consuming to collect and annotate.
\item Encourages more generalizable and robust representations of the data since they capture the inherent structure and properties of the data rather than relying on human-provided labels.
\end{enumerate} 

\subsubsection{Known and unknown invariance}

In pattern recognition, invariance refers to the property of a model or algorithm being robust to certain transformations or changes in the input data. In other words, an invariant model should produce the same or similar output for different instances of the same object, even if they have undergone some changes.

Known invariance: Known invariances are transformations or changes that the model or algorithm is explicitly designed to be robust against. This can include the input data's rotations, translations, scaling, or other known perturbations. By incorporating these invariances into the model's design, the algorithm is better equipped to generalize and recognize patterns in the presence of these transformations.

Unknown invariance: Unknown invariances are transformations or change the model has not been explicitly designed to handle. However, it still manages to be robust due to its learning capabilities. These invariances can emerge from the data and may not be known a priori. For example, a deep learning model might automatically learn to be invariant to certain lighting conditions or other factors not explicitly specified in its design.

\begin{equation}
\label{eq:invariance_to_particular_transformation_function}
\forall x \in X, \forall y \in Y, \forall \rho \in \mathcal{T}, f(x) = f(y) \implies f(\rho(x)) = f(\rho(y))
\end{equation}

In this expression \ref{eq:invariance_to_particular_transformation_function}, $X$ and $Y$ represent sets of input data, $\rho$ represents a transformation function, and $f$ represents a machine learning model. The expression states that for all inputs $x$ and $y$ in $X$ and $Y$, if $f(x) = f(y)$, then $f(\rho(x)) = f(\rho(y))$.

The expression specifies that a model is invariant to a particular transformation function $\rho$ if it produces the same output for both the original and transformed input.


\subsection{Contrastive Learning of Musical Representations (CLMR)}

Contrastive Learning of Musical Representations (CLMR) \cite{CLMR2021} aims to learn discriminative and valuable representations of music without relying on explicit labels. By leveraging the structure and content of music and contrasting different augmentations or transformations of the same musical piece against randomly generated pieces, CLMR models can effectively capture the underlying known invariances that are explicitly incorporated into the design of a model to improve its robustness and generalization. 

In contrast, the model learns unknown invariances from the data without explicit instruction.

The CLMR methodology involves the following steps:

\begin{enumerate}
\item \textbf{Data Augmentation:} Create different augmentations or transformations of the same musical piece as positive pairs and use other randomly selected pieces as antagonistic pairs.
\item \textbf{Contrastive Learning:} Train by minimizing a contrastive loss function, encouraging the model to produce similar positive and dissimilar representations for antagonistic pairs.
\item\textbf{Latent Representation:} Utilize the learned representations for various MIR downstream tasks, such as music genre classification or similarity, both in the symbolic and time domains.
\end{enumerate} 


\subsection{Siamese Networks}
\subsection{Triplet Siamese Networks}
\subsection{XXX}
\subsection{XXX}

\subsection{Encoder}
\subsubsection{SampleCNN}

The SampleCNN model \cite{CLMR2021} takes in 1D input data with a single channel. The first layer is a 1D convolutional layer with a kernel size of 3, a stride of 3, and 128 output channels. It is followed by batch normalization and ReLU activation.

After the initial layer, there are nine hidden layers with varying kernel sizes, strides, and channels. Each hidden layer consists of the following:

\begin{itemize}
    \item A 1D convolutional layer with the specified number of input and output channels, kernel size, a stride of 1, and padding of 1.

    \begin{equation}
y_{c_o}(n) = \sum_{c_i=1}^{C_{\text{in}}} \sum_{k=-\lfloor K/2 \rfloor}^{\lfloor K/2 \rfloor} x_{c_i}(n - k) \cdot w_{c_o, c_i}(k)
\end{equation}

where:

$y_{c_o}(n)$ is the output for the $c_o$-th output channel at position $n$
$C_{\text{in}}$ is the number of input channels
$x_{c_i}(n)$ is the input for the $c_i$-th input channel at position $n$
$w_{c_o, c_i}(k)$ is the kernel weight for the $c_o$-th output channel and $c_i$-th input channel at position $k$
$K$ is the kernel size
The stride is set to 1, and padding is set to 1 (equal padding $\lfloor K/2 \rfloor$ on both sides)
s
    \item Batch normalization. We process a batch of data containing anchor, positive, and negative samples. We pad the waveforms to the same  maximum length among the collection, perform online triplet mining to find the hardest negatives and return the padded anchors, positives, and hardest negatives.

    Let $A$, $P$, and $N$ represent the anchor, positive, and negative samples. The maximum array length in the batch is given by:

    \begin{equation}
L_{\text{max}} = \max_{\text{item} \in \text{batch}}(\max(\text{length}(A_{\text{item}}), \text{length}(P_{\text{item}}), \text{length}(N_{\text{item}})))
\end{equation}

    The anchor-positive distance and the anchor-negative distance are computed as the Euclidean distance between the corresponding samples:

    \begin{equation}
D_{\text{AP}} = \sqrt{\sum_{i} (A_i - P_i)^2}
\end{equation}

    The "hardest" negative index for each anchor-positive pair is determined by maximizing the difference between the anchor-negative and anchor-positive distances:

    \begin{equation}
D_{\text{AN}} = \sqrt{\sum_{i} (A_i - N_i)^2}
\end{equation}

    
    \item ReLU activation. \input{figures/equations/ReLU}

    
    \item Max pooling with kernel size and stride equal to the stride value of the current layer.
\end{itemize}

\begin{tabular}{|c|c|c|}
\hline
\textbf{Layer} & \textbf{Input Channels} & \textbf{Output Channels} \\
\hline
1 & 128 & 128 \\
\hline
2 & 128 & 128 \\
\hline
3 & 128 & 256 \\
\hline
4 & 256 & 256 \\
\hline
5 & 256 & 256 \\
\hline
6 & 256 & 256 \\
\hline
7 & 256 & 256 \\
\hline
8 & 256 & 256 \\
\hline
9 & 256 & 512 \\
\hline
\end{tabular}

After the hidden layers, there's another 1D convolutional layer with 512 input channels, 512 output channels, a kernel size of 3, a stride of 1, and padding of 1. This is followed by batch normalization and ReLU activation.

The output of the final convolutional layer is passed through an average pooling operation across the temporal dimension (dim=2).

If the model is "supervised", dropout with a rate of 0.5 is applied; a regularization technique randomly drops out neurons during training to prevent overfitting, with a 0.5 rate meaning half of the neurons are dropped out.

\input{figures/neural networks/dropout}

Finally, a linear layer with 512 input features and 128 output dimension features is applied to generate the embeddings.

\input{figures/my model/SampleCNN}

We have adapted the original architecture for our experiments to our purpose, which consists of the final average pooling step.

\begin{equation}
y_i = \frac{1}{N} \sum_{j=1}^{N} x_{ij}
\end{equation}

\begin{itemize}
  \item \(y_i\) represents the output tensor after the average pooling operation,
  \item \(x_{ij}\) represents the elements in the 2D input tensor,
  \item \(N\) is the number of elements along the second dimension of the input tensor,
  \item \(i\) iterates over the first dimension of the tensors, and
  \item \(j\) iterates over the second dimension of the input tensor.
\end{itemize}


\subsection{Audio augmentation and transformation pipeline}
\subsubsection{Positive sample generation}
Let $W$ be the input waveform and $W_p$ be the generated positive waveform. The pipeline applies audio effects and adds noise with a random SNR. Define the following random variables for the effects:

\begin{equation}
gain \sim \text{Uniform}(-12, 0)
\end{equation}
\begin{equation}
pitch \sim \text{Uniform}(-1200, 1200)
\end{equation}
\begin{equation}
reverb\_params \sim \text{Uniform}(0, 100)^3
\end{equation}
\begin{equation}
chorus\_params = \begin{cases}
\text{Uniform}(0.1, 1.0), & i=1,2, \\
\text{Uniform}(20, 55), & i=3, \\
\text{Uniform}(0.1, 0.9), & i=4, \\
\text{Uniform}(0.1, 2.0), & i=5, \\
\text{Uniform}(2, 5), & i=6, \\
\text{Categorical}(\{-s, -t\}), & i=7,
\end{cases}
\end{equation}
\begin{equation}
drive \sim \text{Uniform}(0, 30)
\end{equation}
\begin{equation}
stretch \sim \text{Uniform}(0.9, 1.1)
\end{equation}
\begin{equation}
speed \sim \text{Uniform}(0.9, 1.1)
\end{equation}
\begin{equation}
tremolo\_speed \sim \text{Uniform}(0.1, 100)
\end{equation}
\begin{equation}
tremolo\_depth \sim \text{Uniform}(1, 101)
\end{equation}
\begin{equation}
snr\_range \sim \text{Uniform}(12, 100)
\end{equation}

Let $N$ be the noise matrix with dimensions [channels, time domain samples] as $W$ for white noise. Calculate signal power $P_s$ and noise power $P_n$:

\begin{equation}
P_s = \sum (W_{ij})^2
\end{equation}
\begin{equation}
P_n = \sum (N_{ij})^2
\end{equation}

Calculate the scaling factor $\alpha$ for noise based on the desired signal-to-noise ratio (SNR):

\begin{equation}
\alpha = \sqrt{\frac{P_s}{P_n \times 10^{\frac{snr\_range}{10}}}}
\end{equation}

Compute the scaled noise matrix $N_s$ and the noisy waveform matrix $W_n$:

\begin{equation}
N_s = \alpha \times N
\end{equation}
\begin{equation}
W_n = W + N_s
\end{equation}

The final positive waveform $W_p$ is obtained with noise added according to the chosen SNR. This pipeline enhances the diversity of the positive samples and aims to improve the model's ability to retrieve content-based similar-sounding samples sharing the same underlying musical language.

\subsubsection{Negative sample generation}

Let $a$ be the original anchor clip and $n$ be the negative clip generated. The steps for generating the negative clip $n$ are as follows:

\begin{enumerate}
\item Calculate the length $L_a$ and duration $T_a$ of the anchor clip $a$:

\begin{equation}
L_a = \text{len}(a)
\end{equation}

\begin{equation}
T_a = \frac{L_a}{f_s}
\end{equation}

where $\text{len}(a)$ returns the shape of the audio clip, and $f_s$ is the sample rate of the audio.

\item Determine the number of chunks $n_c$ to split the anchor clip into based on a minimum chunk duration $T_c$:

\begin{equation}
n_c = \left\lfloor\frac{T_a}{T_c}\right\rfloor
\end{equation}

\item Calculate the minimum and maximum chunk lengths in samples:

\begin{equation}
L_{c,\text{min}} = T_c \times f_s
\end{equation}
\begin{equation}
L_{c,\text{max}} = T_{\text{max}} \times f_s
\end{equation}

where $T_{\text{max}}$ is the maximum chunk duration specified in seconds.

\item Generate $n_c$ chunk lengths $L_{c,1},\ldots,L_{c,n_c}$ as follows:

\begin{equation}
L_{c,i} = \begin{cases}
L_{c,\text{min}}, & i=1, \\
\text{randint}(L_{c,\text{min}}, L_{c,\text{max}}), & 1<i<n_c, \\
L_a - \sum_{j=1}^{i-1} L_{c,j}, & i=n_c.
\end{cases}
\end{equation}

\item Split the anchor clip into chunks $c_1,\ldots,c_{n_c}$ based on the lengths $L_{c,1},\ldots,L_{c,n_c}$:

\begin{equation}
c_i = a[..., s_i : s_i + L_{c,i}]
\end{equation}
\begin{equation}
S_c = [0] + [L_{c,1},...,L_{c,n_c-1}]
S_c = \sum_{i=0}^{n_c-1} S_{c,i}
\end{equation}
\begin{equation}
s_i = S_c[i]
\end{equation}
\begin{equation}
i=1,\ldots,n_c
\end{equation}

\item Shuffle the resulting list of chunks randomly to obtain the reordered chips $c'_1,\ldots,c'_{n_c}$:

Let $p$ be a random permutation of the indices $1$ to $n_c$. Then, the shuffled array $c'$ is given by:
\begin{equation}
c' = [c_{p(1)}, c_{p(2)}, \dots, c_{p(n_c)}]
\end{equation}


\item Let $c'_1, \dots, c'_{n_c}$ be the shuffled chunks. Then, the negative clip $n$ is obtained by concatenating the shuffled chunks:
\begin{equation}
n = [c'_1, \dots, c'_{n_c}]
\end{equation}


\end{enumerate}

\subsection{Loss function}

Triplet loss was initially proposed and used by Google to learn face recognition of the same person at different poses and angles. \cite{Schroff2015FaceNet:Clustering}

The mathematical expression for the triplet loss function with a variable margin can be written as follows:

\begin{equation}
L(\mathbf{a}, \mathbf{p}, \mathbf{n}) = \frac{1}{N} \sum_{i=1}^{N} \max \left(0, m + \left| \mathbf{a}_i - \mathbf{p}_i \right|_2^2 - \left| \mathbf{a}_i - \mathbf{n}_i \right|_2^2 \right)
\end{equation}

In the triplet loss function, $d(\mathbf{a}, \mathbf{p})$ and $d(\mathbf{a}, \mathbf{n})$ denote the Euclidean distance (L2-norm) between the anchor $\mathbf{a}$ and the positive sample $\mathbf{p}$ and the negative sample $\mathbf{n}$, respectively. The margin $m$ is a positive constant determining the minimum distance between the anchor-positive and anchor-negative pairs.

The $\max(0, ...)$ function computes the maximum of the expression within the parentheses and 0. This function ensures that the loss is only calculated for triplets that violate the margin constraint, where the negative sample is closer to the anchor than the positive sample by at least the margin. The loss is zero when the distance between the positive and negative samples exceeds the margin. Otherwise, the loss is proportional to the extent to which the constraint is violated.

The final form of the loss function is obtained by averaging the loss over a mini-batch of triplets. 

\begin{equation}
L = \frac{1}{N} \sum_{i=1}^{N} L(anchor_i, positive_i, negative_i)
\end{equation}

Here, $N$ is the number of triplets in the mini-batch, and $L(anchor_i, positive_i, negative_i)$ represents the loss function computed for the i-th triplet in the batch.