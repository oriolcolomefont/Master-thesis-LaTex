\section{Model architecture and training strategy}
Piaget's theory of cognitive development \cite{Huitt2003PiagetsDevelopment} posits that children acquire knowledge from sensory and motor experiences in their early stages, from birth to around 18 months. This period called the \textit{sensorimotor stage}, sees the emergence of early representational thought through basic actions such as sucking, grasping, looking, and listening. As children progress through different developmental stages until adolescence and adulthood, their reasoning progressively moves toward abstract ideas and deductive logic. Schemas, higher-order cognitive structures hypothesized to underlie many aspects of human knowledge and skill, emerge during this process, which Piaget interprets through an equilibration mechanism that balances new information with old knowledge. This mechanism involves \textit{assimilation} and \textit{accommodation}, which entail taking in further information to fit pre-existing schemas and modifying existing schemas resulting from the latest news. In the context of learning, complex structures are based on simpler ones, and knowledge transfer occurs when dynamic systems capable of generalization are created. \cite{audioselfsupsurvey}

There are similarities between assimilation and accommodation in Piaget's theory of cognitive development and backpropagation in DL, both of which involve modifying existing structures based on new information. However, it should be noted that these processes' underlying mechanisms and contexts are distinct. While assimilation and accommodation involve modifying mental structures to incorporate new information, backpropagation entails adjusting neural network weights to improve output predictions. Assimilation and accommodation occur in cognitive development, while backpropagation is specific to machine learning. Thus, while these processes have broad similarities, they are fundamentally different and cannot be directly compared in detail.

\subsection{Contrastive Self-Supervised Learning of Musical Representations (CSSLMR)}

Contrastive Self-Supervised Learning of Musical Representations (CSSLMR) is an approach that combines the principles of Self-Supervised Learning (SSL) \cite{audioselfsupsurvey} and Contrastive Learning of Musical Representations (CLMR) \cite{CLMR2021} to learn discriminative and valuable representations of music without relying on explicit labels. By leveraging the structure and content of music and contrasting different augmentations or transformations of the same musical piece against randomly (or not) selected pieces, CSSLMR models can effectively capture the underlying structure and known invariance in the data.

The CSSLMR methodology involves the following steps:

\begin{enumerate}
\item \textbf{Data Augmentation:} Create different augmentations or transformations of the same musical piece as positive pairs and use other randomly selected pieces as antagonistic pairs.
\item \textbf{Contrastive Learning:} Train a neural network to minimize a contrastive loss function, encouraging the model to produce similar representations for positive and dissimilar representations for antagonistic pairs.
\item\textbf{Latent Representation:} Utilize the learned representations for various MIR downstream tasks, such as music genre classification or similarity, both in the symbolic and time domains.
\end{enumerate} 

\subsubsection{Strengths}

CSSLMR inherits the strengths of both SSL and CLMR:

\begin{enumerate}
\item Reduces dependency on labeled data: by leveraging inductive bias (assuming a general invariance across the data samples), CSSL can learn from large amounts of unlabeled data.
\item Encourages feature extraction: CSSLMR promotes learning practical and discriminative representations.
\end{enumerate} 

\section{Model architecture}

We propose using a Triple Siamese Network to train a model for music similarity retrieval \cite{contentmusicsimtriplet2020}. Specifically, we aim to minimize the loss function between an anchor, a positive, and a negative sample by using online triplet mining \cite{Sikaroudi2020OfflinePatches}.

Using the proposed triple siamese network and online triplet mining, we aim to train a model that can effectively distinguish between similar and dissimilar samples while considering their underlying composition and production texture.


\subsection{Encoder}
\subsubsection{SampleCNN}

The SampleCNN model takes in 1D input data with a single channel. The first layer is a 1D convolutional layer with a kernel size of 3, stride of 3, and 128 output channels. It is followed by batch normalization and ReLU activation.

After the initial layer, there are 9 hidden layers with varying kernel sizes, strides, and channels according to the values specified in self.strides and self.hidden. Each hidden layer consists of:

A 1D convolutional layer with the specified number of input and output channels, kernel size, stride of 1, and padding of 1.
Batch normalization.
ReLU activation.
Max pooling with kernel size and stride equal to the stride value of the current layer.
After the hidden layers, there's another 1D convolutional layer with 512 input channels, 512 output channels, kernel size of 3, stride of 1, and padding of 1. This is followed by batch normalization and ReLU activation.

The output of the final convolutional layer is passed through an average pooling operation across the temporal dimension (dim=2).

If the model is in supervised mode, dropout with a rate of 0.5 is applied.

Finally, a linear layer with 512 input features and 128 output dimension features is applied to generate the embeddings.

\input{figures/my model/SampleCNN}

Layer 1:

Input channels: 1
Output channels: 128
Layer 2:

Input channels: 128
Output channels: 128
Layer 3:

Input channels: 128
Output channels: 128
Layer 4:

Input channels: 128
Output channels: 128
Layer 5:

Input channels: 128
Output channels: 256
Layer 6:

Input channels: 256
Output channels: 256
Layer 7:

Input channels: 256
Output channels: 256
Layer 8:

Input channels: 256
Output channels: 256
Layer 9:

Input channels: 256
Output channels: 256
Layer 10:

Input channels: 256
Output channels: 256
Layer 11:

Input channels: 256
Output channels: 256
Layer 12:

Input channels: 256
Output channels: 512
Layer 13:

Input channels: 512
Output channels: 512
Layer 14:

Input channels: 512
Output channels: 128


\subsection{Audio augmentation and transformation pipeline}
The positive samples are created by transforming and augmenting the original samples while preserving their perceptual underlying musical language. This is achieved using techniques such as time-stretching, pitch-shifting, and adding noise to the samples. By doing so, we aim to enhance the diversity of the positive samples and improve the model's ability to retrieve similar-sounding samples that share the same underlying musical language.

On the other hand, the negative samples are created by shuffling and scrambling the original samples while maintaining their production texture. This is achieved using random segment selection, time-domain filtering, and frequency-domain masking. The goal is to retain the texture production of the samples while disrupting their underlying composition.

\subsection{Loss function}

Implements the triplet loss function, commonly used for learning embeddings in metric learning problems. The triplet loss aims to ensure that the distance between an anchor and a positive sample (both from the same class) is smaller than the distance between the anchor and a negative sample (from a different class) by a margin.

$\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N} \max \left( d\left(a_i, p_i\right) - d\left(a_i, n_i\right) + m, 0 \right)$

In this equation:

$N$ is the number of triplets in the batch
$a_i$ is the $i$-th anchor
$p_i$ is the $i$-th positive sample
$n_i$ is the $i$-th negative sample
$d(\cdot, \cdot)$ is the Euclidean distance function
$m$ is the margin; commonly set to 0.2 by default
The forward method computes the triplet loss as follows:

Calculate the pairwise Euclidean distance between the anchor and positive samples, denoted as $d(a_i, p_i)$.
Calculate the pairwise Euclidean distance between the anchor and negative samples, denoted as $d(a_i, n_i)$.
Compute the loss for each triplet in the batch by subtracting the negative distance from the positive distance and adding the margin: $d(a_i, p_i) - d(a_i, n_i) + m$.
Apply the ReLU function, which is equivalent to taking the maximum value between the computed value and 0: $\max \left( d(a_i, p_i) - d(a_i, n_i) + m, 0 \right)$.
Calculate the mean of the losses across all triplets in the batch.