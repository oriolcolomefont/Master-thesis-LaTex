\section{Model architecture and training strategy}
Self-Supervised Learning (SSL) is a technique to learn latent representations from large-scale data without relying on human annotations. Instead, it solves pretext tasks by creating different views of an object that are highly correlated. An SSL model is then trained to generalize objects' representations in a latent high-dimensional space. To achieve this, the model contrasts the representations of the same object with other objects (defined as negative samples) during training. By doing so, a contrastive SSL model is expected to produce more distinctive representations. 

\section{}

We propose using a Triple Siamese Network to train a model for music similarity retrieval. Specifically, we aim to minimize the loss function between the anchor, positive, and negative samples by using online triplet mining.

The positive samples are created by transforming and augmenting the original samples while preserving their perceptual underlying musical language. This is achieved using techniques such as time-stretching, pitch-shifting, and adding noise to the samples. By doing so, we aim to enhance the diversity of the positive samples and improve the model's ability to retrieve similar-sounding samples that share the same underlying musical language.

On the other hand, the negative samples are created by shuffling and scrambling the original samples while maintaining their production texture. This is achieved using random segment selection, time-domain filtering, and frequency-domain masking. The goal is to maintain the production texture of the samples while disrupting their underlying composition.

Using the proposed triple siamese network and online triplet mining, we aim to train a model that can effectively distinguish between similar and dissimilar samples while considering their underlying composition and production texture.

 Implements the triplet loss function, commonly used for learning embeddings in metric learning problems. The triplet loss aims to ensure that the distance between an anchor and a positive sample (both from the same class) is smaller than the distance between the anchor and a negative sample (from a different class) by a margin.

$\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N} \max \left( d\left(a_i, p_i\right) - d\left(a_i, n_i\right) + m, 0 \right)$

In this equation:

$N$ is the number of triplets in the batch
$a_i$ is the $i$-th anchor
$p_i$ is the $i$-th positive sample
$n_i$ is the $i$-th negative sample
$d(\cdot, \cdot)$ is the Euclidean distance function
$m$ is the margin; in this case, it is set to 0.2 by default
The forward method computes the triplet loss as follows:

Calculate the pairwise Euclidean distance between the anchor and positive samples, denoted as $d(a_i, p_i)$.
Calculate the pairwise Euclidean distance between the anchor and negative samples, denoted as $d(a_i, n_i)$.
Compute the loss for each triplet in the batch by subtracting the negative distance from the positive distance and adding the margin: $d(a_i, p_i) - d(a_i, n_i) + m$.
Apply the ReLU function, which is equivalent to taking the maximum value between the computed value and 0: $\max \left( d(a_i, p_i) - d(a_i, n_i) + m, 0 \right)$.
Calculate the mean of the losses across all triplets in the batch.