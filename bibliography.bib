%%%%%%% MSA Overview %%%%%%%
@article{Nieto2020,
   abstract = {With recent advances in the field of music informatics, approaches to audio-based music structural analysis have matured considerably, allowing researchers to reassess the challenges posed by the task, and reimagine potential applications. We review the latest breakthroughs on this topic and discuss the challenges that may arise when applying these techniques in real-world applications. Specifically, we argue that it could be beneficial for these systems to be application-dependent in order to increase their usability. Moreover, in certain scenarios, a user may wish to decide which version of a structure to use, calling for systems with multiple outputs, or where the output adapts in a user-dependent fashion. In reviewing the state of the art and discussing the current challenges on this timely topic, we highlight the subjectivity, ambiguity, and hierarchical nature of musical structure as essential factors to address in future work.},
   author = {Oriol Nieto and Gautham J. Mysore and Cheng-i Wang and Jordan B. L. Smith and Jan Schlüter and Thomas Grill and Brian McFee},
   doi = {10.5334/tismir.54},
   issue = {1},
   journal = {Transactions of the International Society for Music Information Retrieval},
   month = {12},
   pages = {246-263},
   publisher = {Ubiquity Press, Ltd.},
   title = {Audio-Based Music Structure Analysis: Current Trends, Open Challenges, and Applications},
   volume = {3},
   year = {2020},
}

@article{Chaki2021,
   abstract = {Audio signal processing is the most challenging field in the current era for an analysis of an audio signal. Audio signal classification (ASC) comprises of generating appropriate features from a sound and utilizing these features to distinguish the class the sound is most likely to fit. Based on the application’s classification domain, the characteristics extraction and classification/clustering algorithms used may be quite diverse. The paper provides the survey of the state-of art for understanding ASC’s general research scope, including different types of audio; representation of audio like acoustic, spectrogram; audio feature extraction techniques like physical, perceptual, static, dynamic; audio pattern matching approaches like pattern matching, acoustic phonetic, artificial intelligence; classification, and clustering techniques. The aim of this state-of-art paper is to produce a summary and guidelines for using the broadly used methods, to identify the challenges as well as future research directions of acoustic signal processing.},
   author = {Jyotismita Chaki},
   doi = {10.1007/s10772-020-09681-3},
   issn = {15728110},
   issue = {4},
   journal = {International Journal of Speech Technology},
   keywords = {Acoustic phonetic approach,Audio signal processing,Classification of audio,Perceptual audio feature,Physical audio feature},
   month = {12},
   pages = {913-955},
   publisher = {Springer},
   title = {Pattern analysis based acoustic signal processing: a survey of the state-of-art},
   volume = {24},
   year = {2021},
}

@misc{Stutz2022,
    author = {David Stutz},
    title = {Collection of LaTeX resources and examples},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/davidstutz/latex-resources}},
    note = {Accessed on 01.14.2023}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% LaTex diagrams %%%%%%%

@inproceedings{choi2017convolutional,
  title={Convolutional recurrent neural networks for music classification},
  author={Choi, Keunwoo and Fazekas, Gy{\"o}rgy and Sandler, Mark and Cho, Kyunghyun},
  booktitle={2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)},
  pages={2392--2396},
  year={2017},
  organization={IEEE}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% Music Theory %%%%%%%

@book{christensen2004rameau,
  title={Rameau and Musical Thought in the Enlightenment},
  author={Christensen, T. and Bent, I.},
  isbn={9780521617093},
  lccn={lc92039886},
  series={Cambridge Studies in Music Theory and Analysis},
  url={https://books.google.es/books?id=170CpILluzUC},
  year={2004},
  publisher={Cambridge University Press}
}

@misc{schenkerdocumentsonline,
  title = {Schenker Documents Online},
  howpublished = {\url{https://schenkerdocumentsonline.org/index.html}},
  note = {Accessed: 2023-02-05}
}

@report{,
   abstract = {In Schenkerian analysis, one seeks to find structural dependences among the notes of a composition and organize these dependences into a coherent hierarchy that illustrates the function of every note. This type of analysis reveals multiple levels of structure in a composition by constructing a series of simplifications of a piece showing various elaborations and prolongations. We present a framework for solving this problem, called IVI, that uses a state-space search formalism. IVI includes multiple interacting components , including modules for various preliminary analyses (harmonic, melodic, rhythmic, and cadential), identifying and performing reductions, and locating pieces of the Ur-satz. We describe a number of the algorithms by which IVI forms, stores, and updates its hierarchy of notes, along with details of the Ursatz-finding algorithm. We illustrate IVI's functionality on an excerpt from a Schubert piano composition , and also discuss the issues of subproblem interactions and the multiple parsings problem.},
   author = {Phillip B Kirlin and Paul E Utgoff},
   title = {ISMIR 2008-Session 3b-Computational Musicology},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Huang2019MusicTG,
  title={Music Transformer: Generating Music with Long-Term Structure},
  author={Cheng-Zhi Anna Huang and Ashish Vaswani and Jakob Uszkoreit and Ian Simon and Curtis Hawthorne and Noam M. Shazeer and Andrew M. Dai and Matthew D. Hoffman and Monica Dinculescu and Douglas Eck},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


%%%%%%% CNN %%%%%%%%

@INPROCEEDINGS{7500246,
  author={Pons, Jordi and Lidy, Thomas and Serra, Xavier},
  booktitle={2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI)}, 
  title={Experimenting with musically motivated convolutional neural networks}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={A common criticism of deep learning relates to the difficulty in understanding the underlying relationships that the neural networks are learning, thus behaving like a black-box. In this article we explore various architectural choices of relevance for music signals classification tasks in order to start understanding what the chosen networks are learning. We first discuss how convolutional filters with different shapes can fit specific musical concepts and based on that we propose several musically motivated architectures. These architectures are then assessed by measuring the accuracy of the deep learning model in the prediction of various music classes using a known dataset of audio recordings of ballroom music. The classes in this dataset have a strong correlation with tempo, what allows assessing if the proposed architectures are learning frequency and/or time dependencies. Additionally, a black-box model is proposed as a baseline for comparison. With these experiments we have been able to understand what some deep learning based algorithms can learn from a particular set of data.},
  keywords={},
  doi={10.1109/CBMI.2016.7500246},
  ISSN={1949-3991},
  month={June},}

  %%%%%%%%%%%%

  @report{,
   abstract = {Recent work in music structure analysis has shown the potential of deep features to highlight the underlying structure of music audio signals. Despite promising results achieved by such representations, dealing with the inherent hierarchical aspect of music structure remains a challenging problem. Because different levels of segmentation can be considered as equally valid, specifically designed representations should be optimized to improve hierarchical structure analysis. In this work, unsupervised learning of such representations using a contrastive approach operating at different timescales is explored. The proposed system is evaluated on flat and multi-level music segmen-tation. By leveraging both time and the hierarchical organization of music structure, we show that the obtained deep embeddings can encode meaningful patterns and improve segmentation at various levels of granularity.},
   author = {Morgan Buisson and Brian Mcfee and Slim Essid and Hélène C Crayencour},
   keywords = {Bengaluru,Dec 2022,Hélène C Crayencour Learning Multi-Level Representa-tions for Hierarchical Music Structure Analysis International Society for Music Information Retrieval (ISMIR),India ï¿¿hal-03780032ï¿¿,Slim Essid},
   title = {Learning Multi-Level Representations for Hierarchical Music Structure Analysis},
   url = {https://hal.archives-ouvertes.fr/hal-03780032},
}
@article{Kim2020,
   abstract = {Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language Processing, this learning paradigm has also found its way into the field of Music Information Retrieval. In order to benefit from deep learning in an effective, but also efficient manner, deep transfer learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained neural network as the basis for a new learning task. The underlying hypothesis is that if the initial and new learning tasks show commonalities and are applied to the same type of input data (e.g., music audio), the generated deep representation of the data is also informative for the new task. Since, however, most of the networks used to generate deep representations are trained using a single initial learning source, their representation is unlikely to be informative for all possible future tasks. In this paper, we present the results of our investigation of what are the most important factors to generate deep representations for the data and learning tasks in the music domain. We conducted this investigation via an extensive empirical study that involves multiple learning sources, as well as multiple deep learning architectures with varying levels of information sharing between sources, in order to learn music representations. We then validate these representations considering multiple target datasets for evaluation. The results of our experiments yield several insights into how to approach the design of methods for learning widely deployable deep data representations in the music domain.},
   author = {Jaehun Kim and Julián Urbano and Cynthia C.S. Liem and Alan Hanjalic},
   doi = {10.1007/s00521-019-04076-1},
   issn = {14333058},
   issue = {4},
   journal = {Neural Computing and Applications},
   keywords = {Multitask learning,Music Information Retrieval,Representation learning},
   month = {2},
   pages = {1067-1093},
   publisher = {Springer},
   title = {One deep music representation to rule them all? A comparative analysis of different representation learning strategies},
   volume = {32},
   year = {2020},
}
@report{,
   abstract = {The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we provide an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we furnish a definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.},
   author = {Dominik Kreuzberger and Niklas Kühl and Sebastian Hirschl},
   keywords = {CI/CD,DevOps,MLOps,Machine Learning,Operations,Workflow Orchestration},
   title = {Machine Learning Operations (MLOps): Overview, Definition, and Architecture},
}
@article{Kim2020,
   abstract = {Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language Processing, this learning paradigm has also found its way into the field of Music Information Retrieval. In order to benefit from deep learning in an effective, but also efficient manner, deep transfer learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained neural network as the basis for a new learning task. The underlying hypothesis is that if the initial and new learning tasks show commonalities and are applied to the same type of input data (e.g., music audio), the generated deep representation of the data is also informative for the new task. Since, however, most of the networks used to generate deep representations are trained using a single initial learning source, their representation is unlikely to be informative for all possible future tasks. In this paper, we present the results of our investigation of what are the most important factors to generate deep representations for the data and learning tasks in the music domain. We conducted this investigation via an extensive empirical study that involves multiple learning sources, as well as multiple deep learning architectures with varying levels of information sharing between sources, in order to learn music representations. We then validate these representations considering multiple target datasets for evaluation. The results of our experiments yield several insights into how to approach the design of methods for learning widely deployable deep data representations in the music domain.},
   author = {Jaehun Kim and Julián Urbano and Cynthia C.S. Liem and Alan Hanjalic},
   doi = {10.1007/s00521-019-04076-1},
   issn = {14333058},
   issue = {4},
   journal = {Neural Computing and Applications},
   keywords = {Multitask learning,Music Information Retrieval,Representation learning},
   month = {2},
   pages = {1067-1093},
   publisher = {Springer},
   title = {One deep music representation to rule them all? A comparative analysis of different representation learning strategies},
   volume = {32},
   year = {2020},
}
@report{,
   abstract = {The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we provide an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we furnish a definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.},
   author = {Dominik Kreuzberger and Niklas Kühl and Sebastian Hirschl},
   keywords = {CI/CD,DevOps,MLOps,Machine Learning,Operations,Workflow Orchestration},
   title = {Machine Learning Operations (MLOps): Overview, Definition, and Architecture},
}
@article{Pons2019,
   abstract = {Pronounced as "musician", the musicnn library contains a set of pre-trained musically motivated convolutional neural networks for music audio tagging: https://github.com/jordipons/musicnn. This repository also includes some pre-trained vgg-like baselines. These models can be used as out-of-the-box music audio taggers, as music feature extractors, or as pre-trained models for transfer learning. We also provide the code to train the aforementioned models: https://github.com/jordipons/musicnn-training. This framework also allows implementing novel models. For example, a musically motivated convolutional neural network with an attention-based output layer (instead of the temporal pooling layer) can achieve state-of-the-art results for music audio tagging: 90.77 ROC-AUC / 38.61 PR-AUC on the MagnaTagATune dataset --- and 88.81 ROC-AUC / 31.51 PR-AUC on the Million Song Dataset.},
   author = {Jordi Pons and Xavier Serra},
   month = {9},
   title = {musicnn: Pre-trained convolutional neural networks for music audio tagging},
   url = {http://arxiv.org/abs/1909.06654},
   year = {2019},
}

@article{Radford2015,
   abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
   author = {Alec Radford and Luke Metz and Soumith Chintala},
   month = {11},
   title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
   url = {http://arxiv.org/abs/1511.06434},
   year = {2015},
}
@report{,
   abstract = {Humans tend to organize perceived information into hierarchies and structures, a principle that also applies to music. Even musically untrained listeners unconsciously analyze and segment music with regard to various musical aspects, for example, identifying recurrent themes or detecting temporal boundaries between contrasting musical parts. This paper gives an overview of state-of-the-art methods for computational music structure analysis, where the general goal is to divide an audio recording into temporal segments corresponding to musical parts and to group these segments into musically meaningful categories. There are many different criteria for segmenting and structuring music audio. In particular, one can identify three conceptually different approaches, which we refer to as repetition-based, novelty-based, and homogeneity-based approaches. Furthermore, one has to account for different musical dimensions such as melody, harmony, rhythm, and timbre. In our state-of-the-art report, we address these different issues in the context of music structure analysis, while discussing and categorizing the most relevant and recent articles in this field.},
   author = {Jouni Paulus and Anssi Klapuri},
   title = {AUDIO-BASED MUSIC STRUCTURE ANALYSIS},
}

@report{,
   abstract = {Recent work in music structure analysis has shown the potential of deep features to highlight the underlying structure of music audio signals. Despite promising results achieved by such representations, dealing with the inherent hierarchical aspect of music structure remains a challenging problem. Because different levels of segmentation can be considered as equally valid, specifically designed representations should be optimized to improve hierarchical structure analysis. In this work, unsupervised learning of such representations using a contrastive approach operating at different timescales is explored. The proposed system is evaluated on flat and multi-level music segmen-tation. By leveraging both time and the hierarchical organization of music structure, we show that the obtained deep embeddings can encode meaningful patterns and improve segmentation at various levels of granularity.},
   author = {Morgan Buisson and Brian Mcfee and Slim Essid and Hélène C Crayencour},
   keywords = {Bengaluru,Dec 2022,Hélène C Crayencour Learning Multi-Level Representa-tions for Hierarchical Music Structure Analysis International Society for Music Information Retrieval (ISMIR),India ï¿¿hal-03780032ï¿¿,Slim Essid},
   title = {Learning Multi-Level Representations for Hierarchical Music Structure Analysis},
   url = {https://hal.archives-ouvertes.fr/hal-03780032},
}
@report{,
   abstract = {The determination of structural boundaries is a key task for understanding the structure of a musical piece, but it is also highly ambiguous. Recently, Convolutional Neural Networks (CNN) trained on spectrogram features and human annotations have been successfully used to tackle the problem, but still fall clearly behind human performance. We expand on the CNN approach by combining spectro-grams with self-similarity lag matrices as audio features, thereby capturing more facets of the underlying structural information. Furthermore, in order to consider the hierarchical nature of structural organization, we explore different strategies to learn from the two-level annotations of main and secondary boundaries available in the SALAMI structural annotation dataset. We show that both measures improve boundary recognition performance, resulting in a significant improvement over the previous state of the art. As a side-effect, our algorithm can predict boundaries on two different structural levels, equivalent to the training data.},
   author = {Thomas Grill and Jan Schï},
   title = {MUSIC BOUNDARY DETECTION USING NEURAL NETWORKS ON COMBINED FEATURES AND TWO-LEVEL ANNOTATIONS},
   url = {http://www.ofai.at/research/impml/},
}
@article{,
   abstract = {The analysis of the structure of musical pieces is a task that remains a challenge for Artificial Intelligence, especially in the field of Deep Learning. It requires prior identification of structural boundaries of the music pieces. This structural boundary analysis has recently been studied with unsupervised methods and \textit\{end-to-end\} techniques such as Convolutional Neural Networks (CNN) using Mel-Scaled Log-magnitude Spectograms features (MLS), Self-Similarity Matrices (SSM) or Self-Similarity Lag Matrices (SSLM) as inputs and trained with human annotations. Several studies have been published divided into unsupervised and \textit\{end-to-end\} methods in which pre-processing is done in different ways, using different distance metrics and audio characteristics, so a generalized pre-processing method to compute model inputs is missing. The objective of this work is to establish a general method of pre-processing these inputs by comparing the inputs calculated from different pooling strategies, distance metrics and audio characteristics, also taking into account the computing time to obtain them. We also establish the most effective combination of inputs to be delivered to the CNN in order to establish the most efficient way to extract the limits of the structure of the music pieces. With an adequate combination of input matrices and pooling strategies we obtain a measurement accuracy $F_1$ of 0.411 that outperforms the current one obtained under the same conditions.},
   author = {Carlos Hernandez-Olivan and Jose R. Beltran and David Diaz-Guerra},
   doi = {10.9781/ijimai.2021.10.005},
   month = {8},
   title = {Music Boundary Detection using Convolutional Neural Networks: A comparative analysis of combined input features},
   url = {http://arxiv.org/abs/2008.07527 http://dx.doi.org/10.9781/ijimai.2021.10.005},
   year = {2020},
}

@report{,
   abstract = {The recognition of boundaries, e.g., between chorus and verse, is an important task in music structure analysis. The goal is to automatically detect such boundaries in audio signals so that the results are close to human annotation. In this work, we apply Convolutional Neural Networks to the task, trained directly on mel-scaled magnitude spectro-grams. On a representative subset of the SALAMI structural annotation dataset, our method outperforms current techniques in terms of boundary retrieval F-measure at different temporal tolerances: We advance the state-of-the-art from 0.33 to 0.46 for tolerances of ±0.5 seconds, and from 0.52 to 0.62 for tolerances of ±3 seconds. As the algorithm is trained on annotated audio data without the need of expert knowledge, we expect it to be easily adaptable to changed annotation guidelines and also to related tasks such as the detection of song transitions.},
   author = {Karen Ullrich and Jan Schlüter and Thomas Grill},
   title = {BOUNDARY DETECTION IN MUSIC STRUCTURE ANALYSIS USING CONVOLUTIONAL NEURAL NETWORKS},
   url = {http://ofai.at/research/impml/projects/},
}
@report{,
   abstract = {Composing musical ideas longer than motifs or 1 figures is still rare in music generated by machine learning 2 methods, a problem that is commonly referred to as the lack of 3 long-term structure in the generated sequences. In addition, the 4 evaluation of the structural complexity of artificial compositions 5 is still a manual task, requiring expert knowledge, time and 6 involving subjectivity which is inherent in the perception of 7 musical structure. Based on recent advancements in music struc-8 ture analysis, we automate the evaluation process by introducing 9 a collection of metrics that can objectively describe structural 10 properties of the music signal. This is done by segmenting 11 music hierarchically, and computing our metrics on the resulting 12 hierarchies to characterise the decomposition process of music 13 into its structural components. We tested our method on a dataset 14 collecting music with different degrees of structural complexity, 15 from random and computer-generated pieces to real compositions 16 of different genres and formats. Results indicate that our method 17 can discriminate between these classes of complexity and identify 18 further non-trivial subdivisions according to their structural 19 properties. Our work contributes a simple yet effective frame-20 work for the evaluation of music generation models in regard to 21 their ability to create structurally meaningful compositions. 22},
   author = {Jacopo De Berardinis and Angelo Cangelosi and Eduardo Coutinho},
   issue = {Y},
   journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING},
   keywords = {Evaluation measures 23,Index Terms-Music structure analysis},
   pages = {1},
   title = {Measuring the structural complexity of music: from structural segmentations to the automatic evaluation of models for music generation},
   volume = {2021},
}
@article{Wang2021,
   abstract = {Music structure analysis (MSA) methods traditionally search for musically meaningful patterns in audio: homogeneity, repetition, novelty, and segment-length regularity. Hand-crafted audio features such as MFCCs or chromagrams are often used to elicit these patterns. However, with more annotations of section labels (e.g., verse, chorus, and bridge) becoming available, one can use supervised feature learning to make these patterns even clearer and improve MSA performance. To this end, we take a supervised metric learning approach: we train a deep neural network to output embeddings that are near each other for two spectrogram inputs if both have the same section type (according to an annotation), and otherwise far apart. We propose a batch sampling scheme to ensure the labels in a training pair are interpreted meaningfully. The trained model extracts features that can be used in existing MSA algorithms. In evaluations with three datasets (HarmonixSet, SALAMI, and RWC), we demonstrate that using the proposed features can improve a traditional MSA algorithm significantly in both intra- and cross-dataset scenarios.},
   author = {Ju-Chiang Wang and Jordan B. L. Smith and Wei-Tsung Lu and Xuchen Song},
   month = {10},
   title = {Supervised Metric Learning for Music Structure Features},
   url = {http://arxiv.org/abs/2110.09000},
   year = {2021},
}
@article{Li2016,
   abstract = {We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches for addressing the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely-used music audio datasets. We anticipate that the dataset will be useful for the development and evaluation of existing music information retrieval (MIR) tasks, as well as for novel multi-modal tasks. We benchmark two existing MIR tasks (multi-pitch analysis and score-informed source separation) on the dataset and compare with other existing music audio datasets. Additionally, we consider two novel multi-modal MIR tasks (visually informed multi-pitch analysis and polyphonic vibrato analysis) enabled by the dataset and provide evaluation measures and baseline systems for future comparisons (from our recent work). Finally, we propose several emerging research directions that the dataset enables.},
   author = {Bochen Li and Xinzhao Liu and Karthik Dinesh and Zhiyao Duan and Gaurav Sharma},
   doi = {10.1109/TMM.2018.2856090},
   month = {12},
   title = {Creating A Multi-track Classical Musical Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications},
   url = {http://arxiv.org/abs/1612.08727 http://dx.doi.org/10.1109/TMM.2018.2856090},
   year = {2016},
}
@report{,
   abstract = {We propose a novel and fast approach to discover structure in western popular music by using a specific type of matrix factorization that adds a convex constrain to obtain a decomposition that can be interpreted as a set of weighted cluster centroids. We show that these centroids capture the different sections of a musical piece (e.g. verse, chorus) in a more consistent and efficient way than classic non-negative matrix factorization. This technique is capable of identifying the boundaries of the sections and then grouping them into different clusters. Additionally, we evaluate this method on two different datasets and show that it is competitive compared to other music segmentation techniques, outperforming other matrix factorization methods.},
   author = {Oriol Nieto and Tristan Jehan},
   keywords = {Index Terms-matrix factorization,music structure analysis,segmentation},
   title = {CONVEX NON-NEGATIVE MATRIX FACTORIZATION FOR AUTOMATIC MUSIC STRUCTURE IDENTIFICATION},
   url = {http://developer.echonest.com},
}
@article{Peeters2022,
   abstract = {In this paper, we propose a new paradigm to learn audio features for Music Structure Analysis (MSA). We train a deep encoder to learn features such that the Self-Similarity-Matrix (SSM) resulting from those approximates a ground-truth SSM. This is done by minimizing a loss between both SSMs. Since this loss is differentiable w.r.t. its input features we can train the encoder in a straightforward way. We successfully demonstrate the use of this training paradigm using the Area Under the Curve ROC (AUC) on the RWC-Pop dataset.},
   author = {Geoffroy Peeters and Florian Angulo},
   month = {11},
   title = {SSM-Net: feature learning for Music Structure Analysis using a Self-Similarity-Matrix based loss},
   url = {http://arxiv.org/abs/2211.08141},
   year = {2022},
}
@report{,
   author = {Farbood P Professor Juan Bello Doctor Tristan Jehan},
   title = {Sponsoring Committee: Professor Morwaread M},
   year = {2015},
}
@article{Zhang2022,
   abstract = {Music tension is a link between music structures and emotions. As music unfolds, developmental patterns induce various emotional experiences, but the relationship between developmental patterns and tension experience remains unclear. The present study compared two developmental patterns of two successive phrases (tonal shift and melodic shift) with repetition condition to investigate the relationship with tension experience. Professional musicians rated on-line felt tension and EEG responses were recorded while listening to music sequences. Behavioral results showed that tension ratings under tonal and melodic shift conditions were higher than those under repetition conditions. ERP results showed larger potentials at early P300 and late positive component (LPC) time windows under tonal shift condition, and early right anterior negativity (ERAN) and LPC under melodic shift condition. ERSP results showed early beta and late gamma power increased under tonal shift condition, theta power decreased and alpha power increased under melodic shift condition. Our findings suggest that developmental patterns play a vital role in tension experiences; tonal shift affects tension by tonal shift detection and integration, while melodic shift affects tension by attentional processing and working memory integration. From the perspective of Event Structure Processing Model, solid evidence was given to specify the time-span segmentation and reduction.},
   author = {Ning Zhang and Lijun Sun and Qiong Wu and Yufang Yang},
   doi = {10.1038/s41598-022-11949-4},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   month = {12},
   pmid = {35585148},
   publisher = {Nature Research},
   title = {Tension experience induced by tonal and melodic shift at music phrase boundaries},
   volume = {12},
   year = {2022},
}
@report{,
   abstract = {We approach the task of automatic music segmentation by musical form structure. After reviewing previous efforts which have achieved good results, we consider the rapidly evolving application of convolutional neural networks (CNNs). As CNNs have revolutionized the field of image recognition, especially since 2012, we investigate the current and future possibilities for such an approach to music, and specifically the task of structure segmentation. We implement a straightforward example of such a system, and discuss its preliminary performance as well as future opportunities. 1 2},
   author = {Tim O'brien},
   title = {MUSICAL STRUCTURE SEGMENTATION WITH CONVOLUTIONAL NEURAL NETWORKS},
   url = {http://www.music-ir.org/mirex/results/},
}
@report{,
   abstract = {Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets-principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.},
   author = {Jort F Gemmeke and Daniel P W Ellis and Dylan Freedman and Aren Jansen and Wade Lawrence and R Channing Moore and Manoj Plakal and Marvin Ritter},
   keywords = {Index Terms-Audio event detection,audio databases,data collection,sound ontology},
   title = {AUDIO SET: AN ONTOLOGY AND HUMAN-LABELED DATASET FOR AUDIO EVENTS},
   url = {http://en.wikipedia.org/wiki/Bird},
}
@inproceedings{Hu2022,
   abstract = {Repetition, a basic form of artistic creation, appears in most musical works and delivers enthralling aesthetic experiences.},
   author = {Zhejing Hu and Xiao Ma and Yan Liu and Gong Chen and Yongxu Liu},
   doi = {10.1145/3503161.3548130},
   month = {10},
   pages = {1223-1231},
   publisher = {Association for Computing Machinery (ACM)},
   title = {The Beauty of Repetition in Machine Composition Scenarios},
   year = {2022},
}
@article{Marmoret2022,
   abstract = {Music Structure Analysis (MSA) consists in segmenting a music piece in several distinct sections. We approach MSA within a compression framework, under the hypothesis that the structure is more easily revealed by a simplified representation of the original content of the song. More specifically, under the hypothesis that MSA is correlated with similarities occurring at the bar scale, this article introduces the use of linear and non-linear compression schemes on barwise audio signals. Compressed representations capture the most salient components of the different bars in the song and are then used to infer the song structure using a dynamic programming algorithm. This work explores both low-rank approximation models such as Principal Component Analysis or Nonnegative Matrix Factorization and "piece-specific" Auto-Encoding Neural Networks, with the objective to learn latent representations specific to a given song. Such approaches do not rely on supervision nor annotations, which are well-known to be tedious to collect and possibly ambiguous in MSA description. In our experiments, several unsupervised compression schemes achieve a level of performance comparable to that of state-of-the-art supervised methods (for 3s tolerance) on the RWC-Pop dataset, showcasing the importance of the barwise compression processing for MSA.},
   author = {Axel Marmoret and Jérémy E. Cohen and Frédéric Bimbot},
   month = {2},
   title = {Barwise Compression Schemes for Audio-Based Music Structure Analysis},
   url = {http://arxiv.org/abs/2202.04981},
   year = {2022},
}
@report{,
   abstract = {Music segmentation refers to the dual problem of identifying boundaries between, and labeling, distinct music segments, e.g., the chorus , verse, bridge etc. in popular music. The performance of a range of music segmentation algorithms has been shown to be dependent on the audio features chosen to represent the audio. Some approaches have proposed learning feature transformations from music segment annotation data, although, such data is time consuming or expensive to create and as such these approaches are likely limited by the size of their datasets. While annotated music segmentation data is a scarce resource, the amount of available music audio is much greater. In the neighboring field of semantic audio unsupervised deep learning has shown promise in improving the performance of solutions to the query-by-example and sound classification tasks. In this work, unsupervised training of deep feature embeddings using convolutional neural networks (CNNs) is explored for music segmentation. The proposed techniques exploit only the time proximity of audio features that is implicit in any audio timeline. Employing these embeddings in a classic music segmentation algorithm is shown not only to significantly improve the performance of this algorithm, but obtain state of the art performance in unsupervised music segmentation.},
   author = {Matthew C Mccallum},
   keywords = {Acoustic signal processing,Convolutional neural network,Deep learning,Index Terms-Music information retrieval},
   title = {UNSUPERVISED LEARNING OF DEEP FEATURES FOR MUSIC SEGMENTATION},
}
@report{,
   abstract = {Structure in music is traditionally analyzed hierarchically: large-scale sections can be subdivided and refined down to the short melodic ideas at the motivic level. However, typical algorithmic approaches to structural annotation produce flat temporal partitions of a track, which are commonly evaluated against a similarly flat, human-produced annotation. Evaluating structure analysis as represented by flat annotations effectively discards all notions of structural depth in the evaluation. Although collections of hierarchical structure annotations have been recently published, no techniques yet exist to measure an algorithm's accuracy against these rich structural annotations. In this work, we propose a method to evaluate structural boundary detection with hierarchical annotations. The proposed method transforms boundary detection into a ranking problem, and facilitates the comparison of both flat and hierarchical annotations. We demonstrate the behavior of the proposed method with various synthetic and real examples drawn from the SALAMI dataset.},
   author = {Brian Mcfee and Oriol Nieto and Juan P Bello},
   title = {HIERARCHICAL EVALUATION OF SEGMENT BOUNDARY DETECTION},
}
@report{,
   abstract = {We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f 0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.},
   author = {Rachel Bittner and Justin Salamon and Mike Tierney and Matthias Mauch and Chris Cannam and Juan Bello},
   title = {MedleyDB: A MULTITRACK DATASET FOR ANNOTATION-INTENSIVE MIR RESEARCH},
   url = {http://marl.smusic.nyu.edu/medleydb},
}
@article{Marmoret2022,
   abstract = {Music Structure Analysis (MSA) consists of representing a song in sections (such as ``chorus'', ``verse'', ``solo'' etc), and can be seen as the retrieval of a simplified organization of the song. This work presents a new algorithm, called Convolutive Block-Matching (CBM) algorithm, devoted to MSA. In particular, the CBM algorithm is a dynamic programming algorithm, applying on autosimilarity matrices, a standard tool in MSA. In this work, autosimilarity matrices are computed from the feature representation of an audio signal, and time is sampled on the barscale. We study three different similarity functions for the computation of autosimilarity matrices. We report that the proposed algorithm achieves a level of performance competitive to that of supervised state-of-the-art methods on 3 among 4 metrics, while being fully unsupervised.},
   author = {Axel Marmoret and Jérémy E. Cohen and Frédéric Bimbot},
   month = {10},
   title = {Convolutive Block-Matching Segmentation Algorithm with Application to Music Structure Analysis},
   url = {http://arxiv.org/abs/2210.15356},
   year = {2022},
}
@report{,
   abstract = {In this work we present a framework containing open source implementations of multiple music structural seg-mentation algorithms and employ it to explore the hy-per parameters of features, algorithms, evaluation metrics, datasets, and annotations of this MIR task. Besides testing and discussing the relative importance of the moving parts of the computational music structure ecosystem , we also shed light on its current major challenges. Additionally, a new dataset containing multiple structural annotations for tracks that are particularly ambiguous to analyze is introduced , and used to quantify the impact of specific anno-tators when assessing automatic approaches to this task. Results suggest that more than one annotation per track is necessary to fully address the problem of ambiguity in music structure research.},
   author = {Oriol Nieto and Juan Pablo Bello},
   title = {SYSTEMATIC EXPLORATION OF COMPUTATIONAL MUSIC STRUCTURE RESEARCH},
   url = {https://github.com/urinieto/msaf},
}
@report{,
   abstract = {Neural networks have been used to learn a latent "musi-cal space" or "embedding" to encode meaningful features and provide a method of measuring semantic similarity between two musical passages. An ideal embedding is one that both captures features useful for downstream tasks and conforms to a distribution suitable for sampling and meaningful interpolation. We present two new methods for learning musical embeddings that leverage context while simultaneously imposing a shape on the feature space distribution via backpropagation using an adversarial component. We focus on the symbolic domain and target short polyphonic musical units consisting of 40 note sequences. The goal is to project these units into a continuous low dimensional space that has semantic relevance. We evaluate relevance based on the learned features' abilities to complete various musical tasks and show improvement over baseline models including variational autoencoders, adver-sarial autoencoders, and deep structured semantic models. We use a dataset consisting of classical piano and demonstrate the robustness of our methods across multiple input representations.},
   author = {Mason Bretan and Larry Heck},
   title = {LEARNING SEMANTIC SIMILARITY IN MUSIC VIA SELF-SUPERVISION},
}
@report{,
   abstract = {We present a query-by-example system for content-based music information retrieval by ranking items in a database based on semantic similarity, rather than acoustic similarity , to a query example. The retrieval system is based on semantic concept models that are learned from the CAL-500 data set containing both audio examples and their text captions. Using the concept models, the audio tracks are mapped into a semantic feature space, where each dimension indicates the strength of the semantic concept. Audio similarity and retrieval is then based on ranking the database tracks by their similarity to the query in the semantic space.},
   author = {Luke Barrington and Douglas Turnbull and David Torres and Gert Lanckriet},
   title = {SEMANTIC SIMILARITY FOR MUSIC RETRIEVAL},
}
@report{,
   abstract = {Automatically inferring the structural properties of raw multimedia documents is essential in today's digitized society. Given its hierarchical and multi-faceted organization, musical pieces represent a challenge for current computational systems. In this article, we present a novel approach to music structure annotation based on the combination of structure features with time series similarity. Structure features encapsulate both local and global properties of a time series, and allow us to detect boundaries between homogeneous, novel, or repeated segments. Time series similarity is used to identify equivalent segments, corresponding to musically meaningful parts. Extensive tests with a total of five benchmark music collections and seven different human annotations show that the proposed approach is robust to different ground truth choices and parameter settings. Moreover, we see that it outperforms previous approaches evaluated under the same framework.},
   author = {Joan Serrà and Meinard Müller and Peter Grosche and Josep Ll Arcos},
   title = {Unsupervised Music Structure Annotation by Time Series Structure Features and Segment Similarity},
   url = {http://www.music-ir.org/mirex/wiki/MIREX},
   year = {2013},
}
@article{Wang2022,
   abstract = {Conventional music structure analysis algorithms aim to divide a song into segments and to group them with abstract labels (e.g., 'A', 'B', and 'C'). However, explicitly identifying the function of each segment (e.g., 'verse' or 'chorus') is rarely attempted, but has many applications. We introduce a multi-task deep learning framework to model these structural semantic labels directly from audio by estimating "verseness," "chorusness," and so forth, as a function of time. We propose a 7-class taxonomy (i.e., intro, verse, chorus, bridge, outro, instrumental, and silence) and provide rules to consolidate annotations from four disparate datasets. We also propose to use a spectral-temporal Transformer-based model, called SpecTNT, which can be trained with an additional connectionist temporal localization (CTL) loss. In cross-dataset evaluations using four public datasets, we demonstrate the effectiveness of the SpecTNT model and CTL loss, and obtain strong results overall: the proposed system outperforms state-of-the-art chorus-detection and boundary-detection methods at detecting choruses and boundaries, respectively.},
   author = {Ju-Chiang Wang and Yun-Ning Hung and Jordan B. L. Smith},
   month = {5},
   title = {To catch a chorus, verse, intro, or anything else: Analyzing a song with structural functions},
   url = {http://arxiv.org/abs/2205.14700},
   year = {2022},
}
@article{Jiang2022,
   abstract = {We propose a novel method to model hierarchical metrical structures for both symbolic music and audio signals in a self-supervised manner with minimal domain knowledge. The model trains and inferences on beat-aligned music signals and predicts an 8-layer hierarchical metrical tree from beat, measure to the section level. The training procedural does not require any hierarchical metrical labeling except for beats, purely relying on the nature of metrical regularity and inter-voice consistency as inductive biases. We show in experiments that the method achieves comparable performance with supervised baselines on multiple metrical structure analysis tasks on both symbolic music and audio signals. All demos, source code and pre-trained models are publicly available on GitHub.},
   author = {Junyan Jiang and Gus Xia},
   month = {10},
   title = {Self-Supervised Hierarchical Metrical Structure Modeling},
   url = {http://arxiv.org/abs/2210.17183},
   year = {2022},
}
@article{He2022,
   abstract = {Chorus detection is a challenging problem in musical signal processing as the chorus often repeats more than once in popular songs, usually with rich instruments and complex rhythm forms. Most of the existing works focus on the receptiveness of chorus sections based on some explicit features such as loudness and occurrence frequency. These pre-assumptions for chorus limit the generalization capacity of these methods, causing misdetection on other repeated sections such as verse. To solve the problem, in this paper we propose an end-to-end chorus detection model DeepChorus, reducing the engineering effort and the need for prior knowledge. The proposed model includes two main structures: i) a Multi-Scale Network to derive preliminary representations of chorus segments, and ii) a Self-Attention Convolution Network to further process the features into probability curves representing chorus presence. To obtain the final results, we apply an adaptive threshold to binarize the original curve. The experimental results show that DeepChorus outperforms existing state-of-the-art methods in most cases.},
   author = {Qiqi He and Xiaoheng Sun and Yi Yu and Wei Li},
   month = {2},
   title = {DEEPCHORUS: A Hybrid Model of Multi-scale Convolution and Self-attention for Chorus Detection},
   url = {http://arxiv.org/abs/2202.06338},
   year = {2022},
}
@article{Wang2022,
   abstract = {Music structure analysis (MSA) systems aim to segment a song recording into non-overlapping sections with useful labels. Previous MSA systems typically predict abstract labels in a post-processing step and require the full context of the song. By contrast, we recently proposed a supervised framework, called "Music Structural Function Analysis" (MuSFA), that models and predicts meaningful labels like 'verse' and 'chorus' directly from audio, without requiring the full context of a song. However, the performance of this system depends on the amount and quality of training data. In this paper, we propose to repurpose a public dataset, HookTheory Lead Sheet Dataset (HLSD), to improve the performance. HLSD contains over 18K excerpts of music sections originally collected for studying automatic melody harmonization. We treat each excerpt as a partially labeled song and provide a label mapping, so that HLSD can be used together with other public datasets, such as SALAMI, RWC, and Isophonics. In cross-dataset evaluations, we find that including HLSD in training can improve state-of-the-art boundary detection and section labeling scores by ~3% and ~1% respectively.},
   author = {Ju-Chiang Wang and Jordan B. L. Smith and Yun-Ning Hung},
   month = {11},
   title = {MuSFA: Improving Music Structural Function Analysis with Partially Labeled Data},
   url = {http://arxiv.org/abs/2211.15787},
   year = {2022},
}
@article{Spijkervet2021,
   abstract = {While deep learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive, and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations to form a simple framework for self-supervised, contrastive learning of musical representations: CLMR. This approach works on raw time-domain music data and requires no labels to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagATune and Million Song datasets and present an ablation study to test which of our music-related innovations over SimCLR are most effective. A linear classifier trained on the proposed representations achieves a higher average precision than supervised models on the MagnaTagATune dataset, and performs comparably on the Million Song dataset. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that our method has strong generalisability in music classification. Lastly, we show that the proposed method allows data-efficient learning on smaller labeled datasets: we achieve an average precision of 33.1 percent despite using only 259 labeled songs in the MagnaTagATune dataset (1% of the full dataset) during linear evaluation. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper.},
   author = {Janne Spijkervet and John Ashley Burgoyne},
   month = {3},
   title = {Contrastive Learning of Musical Representations},
   url = {http://arxiv.org/abs/2103.09410},
   year = {2021},
}

